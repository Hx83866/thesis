In this chapter, the performance of the text-aware process prediction model is evaluated based on simulated and real-world event data.
First, the data sets and evaluation method are described. Then, the performance of differently parameterized models on the data sets is evaluated and analyzed in-depth.


\section{Data Sets}

The prediction model is evaluated on real-world and simulated event logs, which are described in the following. An overview of the key properties of the logs is summarized in Table \ref{tab:logs}.

\textbf{Job Application} (Simulated) This event log describes a simple job application process. 
First, the applicant submits an application in the system.
Then, a curriculum vitae (CV) and a cover letter are uploaded in an random order, where the CV is uploaded first with 75\% probability.
When both documents have been received, the applicant is either directly rejected or invited to an interview.
After the interview a decision is made and the applicant gets an job offer or is rejected.
The CV contains text information about the skills of the applicant.
The probability for an interview invitation is exactly the relative share of technology related skills.
After the interview, an email is sent to the applicant with the final decision.
After that, the applicant gets an job offer or is rejected.
Furthermore, noise is added by introducing a 3\% probability that the applicant will stop the process after every event.
The time of each event is determined by sampling from a normal distribution, which is unique per activity.
The text information in the CV and the email is generated by sampling multiple times from sets of sentences.
For example, if an applicant gets a job offer, the generated email contains text fragments from typical job offer emails.
With this text generation mechanism, all texts in the event log are unique, but the words and sentences in the texts correlate with the path of the corresponding case.

\textbf{werk.nl} (Real-world) This event log describes customer's journeys of the Employee Insurance Agency commissioned by the Dutch Ministry of Social Affairs and Employment. The log is aggregated from two anonymized data sets, that were provided in the BPI Challenge 2016 \cite{bpichallenge2016}, containing click data of logged in customers from the official website werk.nl and phone call data from the call center.
Both data sets are joined based on the customer ID to derived a detailed view on customer contacts.
For each phone call the costumer's question is available as a text attribute in English. In addition, the customer's age (grouped) and gender is considered as additional attributes.
The event log is filtered to remove outlier activities (threshold <0.5\%) and infrequent trace variants (2 or less traces with the same variant). After prepossessing, 18 distinct activities and 1001 trace variants remain, such that the process can be described as quite unstructured.

 
\textbf{Help Desk} (Real-world)

\begin{table}[!htbp]
	\begin{tabularx}{\textwidth}{l r r r}
		\toprule
		\textbf{Data Set} & \textbf{Job Application} & \textbf{werk.nl} &\textbf{Help Desk}  \\
		\midrule
		Number of cases & 20\,000& 15\,001& \\
		Number of trace variants &13 & 1\,001 & \\
		Number of events & 125\,752 & 55\,220 & \\
		Events per case (mean) & 6.288& 3.681& \\
		Median case duration (days) & 5.063 & 0.224& \\
		Mean case duration (days)&4.54 &  0.713 & \\
		Number of activities & 9 & 18 & \\
		Number of words before preprocessing & 2\,870\,230 &247\,010 & \\
		Number of words after preprocessing  &1\,391\,458 &98\,915 & \\
		Vocabulary size before preprocessing &166 & 1\,203 & \\
		Vocabulary size after preprocessing & 121 & 815 & \\
		\bottomrule
	\end{tabularx}
	\caption[Overview of evaluated data sets]{Overview of evaluated data sets.}
	\label{tab:logs}
\end{table}

\section{Evaluation Method}

Each event log is evaluated in a consistent procedure.
In the first step, the event log is separated into a training and test log. 
The training log consists of the first 2/3 chronologically ordered traces and is used to fit the prediction model to the historical data.
The remaining 1/3 of traces are used to measure the prediction performance.
For each trace $\sigma$ in the test log, all prefixes $hd^k(\sigma)$ of length $2 \leq k \leq |\sigma| - 1$ are considered as instances for prediction.
For prefix traces with only one event, predictions seem to be less stable as also observed in \cite{DBLP:conf/caise/TaxVRD17} and are therefore not included for the metric computation.
The LSTM network is trained with at most 100 epochs and the learning rate is initialized with 0.001.
During the training of the LSTM model, 20\% of the training log is used for validation: If the error on the validation log is not decreasing anymore for 5 epochs, the training rate is reduced, and if the error is not decreased for 10 epochs, the training is stopped in order to avoid overfitting.
Furthermore, the LSTM layers use dropout \cite{DBLP:journals/corr/abs-1207-0580} of 20\% during training as an additional measure against overfitting.

For classification (i.e. categorical prediction) task, like next event and outcome prediction, the accuracy is utilized as metric.
The accuracy is computed as the number of correct predictions $t$ divided by the total number of predictions $n$, i.e. 
\begin{equation*}
	\textrm{accuracy} = \dfrac{t}{n} = \dfrac{\textrm{\# correct predictions}}{\textrm{\# total predictions}} \in [0,1].
\end{equation*}

For regression tasks, like the next event time and the case cycle time predictions, the mean absolute error (MAE) is computed to measure the prediction performance. The mean absolute error indicates the average absolute difference between the predicted value $\hat{y}$ and the true value $y$,  precisely
\begin{equation*}
	\textrm{MAE} = \dfrac{1}{n}\sum_{i=1}^{n}|\hat{y_i} - y_i| \in [0, \infty).
\end{equation*}
This error metric is favored, since it gives a more intuitive interpretation and is less sensitive to outliers compared to similar metrics like the mean squared error (MSE).
An accuracy of 1 and a MAE of 0 is the most desirable.

The prediction model is evaluated with each presented text model and different encoding lengths for the text.
For the Bag of Words and Bag of N-Gram model the encoding length is adjusted by only considering the most frequent terms in the vocabulary.
The encoding dimension of the non-textual data depends on the considered attributes and their number of the distinct values in the event logs.
The Bag of N-Gram model is used with bigrams ($n=2$).
The Paragraph Vector model is trained for 15 epochs.

\section{Next Event Prediction}

\begin{table}[!htbp]
	\newrobustcmd{\B}{\fontseries{b}\selectfont}
	\setlength\tabcolsep{3pt}
	\begin{tabularx}{\textwidth}{
			>{\hsize=1.0\hsize}X
			>{\hsize=1.0\hsize}X
			>{\hsize=1.0\hsize}X
			>{\hsize=1.0\hsize}X
			>{\hsize=1.0\hsize}X
			>{\hsize=1.0\hsize}X
			>{\hsize=1.0\hsize}X
			>{\hsize=1.0\hsize}X
		}
		\toprule
		& & \multicolumn{2}{l}{\textbf{Job Application}} & \multicolumn{2}{l}{\textbf{werk.nl}} & \multicolumn{2}{l}{\textbf{Help Desk}} \\
		Text & Text &Activity & Time & Activity& Time  & Activity& Time  \\
		Model & Dimension &Accuracy & MAE & Accuracy& MAE  & Accuracy& MAE  \\
		\midrule
		\multicolumn{8}{c}{\textit{Text-Aware Process Prediction (LSTM)}} \\
		BoW&50&    \B 0.9919&     0.0743&     0.5274&     0.0778 \\
		BoW&100& \B   0.9919&     0.0753& \B    0.5293&  \B   0.0776 \\
		BoW&500&  \B   0.9919&     0.0737&     0.5229&     0.0785 \\
		BoNG&50&     0.9918&     0.0743&     0.5288&     0.0788 \\
		BoNG&100& \B    0.9919&     0.0735&     0.5242&     0.0798 \\
		BoNG&500& \B    0.9919&     0.0738&     0.5214&    0.0782 \\
		PV&10&     0.9216&     0.1633&     0.5153&     0.0785 \\
		PV&20&     0.9759&     0.0939&     0.5206&     0.0789 \\
		PV&100&     0.9916&     0.0760&     0.5199&     0.0785 \\
		LDA&10& \B    0.9919&  \B   0.0734&     0.5244&     0.0786 \\
		LDA&20& \B    0.9919&     0.0756&     0.5214&     0.0782 \\
		LDA&100&  \B   0.9919&     0.0762&     0.5271&     0.0788 \\
		\multicolumn{8}{c}{\textit{LSTM baseline}} \\
		-&0&     0.8733&     0.2122&     0.5128&     0.0785 \\
		\bottomrule
	\end{tabularx}
	\caption[Experimental results for the next event prediction]{Experimental results for the next event prediction.}
	\label{tab:next-event}
\end{table}

First, the prediction performance regarding the activity and timestamp of the next event is evaluated.
We compare the text-aware prediction model to the LSTM baseline, that only considers the activity, timestamp and additional non-textual data.
The results are summarized in Table \ref{tab:next-event}.
Each line in the table states the prediction accuracy for the next activity and the mean absolute error in days for the next timestamp prediction of a single model on all evaluated event logs.
Each text-aware model is tested with 3 different encoding lengths for the text.
The BoW and BoNG models are evaluated with 50, 100 and 500 dimensional text vectors, the PV and LDA models with smaller text encodings of size 10, 20 and 100.

Compared to the LSTM baseline, the text-aware model is able to improve the prediction performance for the next activity and timestamp on all data sets.
However, the effect of textual data is much bigger on the simulated data set compared to the real-world data sets.
The textual data in the simulated log correlates with the control flow by design, in the real-world data the correlation is just assumed.

The choice of text model has a rather small impact on the results.
Notably, the BoW and LDA model perform most consistently on all data sets and all text encoding lengths.
The PV model reaches similar results using a hundred dimensional text vector, but performs worse when a small text embedding size of 10 or 20 is used.
The BoNG model has a similar performance to BoW model.
The word order awareness of the first does not convert to better prediction results.

In addition, the prediction performance is evaluated per prefix length for each event log.
Figure \ref{fig:next-activity-prefix} shows the next activity prediction accuracy and next timestamp MAE.
On the job application event log all text-aware process models almost perfectly predict the activity.
The LSTM baseline accuracy drops down to around 75\% at every decision point in the process, since it can not take advantage of the documents, but simply predicts the most likely next activity.
Similarly, the timestamp prediction error of the LSTM baseline jumps up at the decision point.
All text-aware approaches can unequivocally utilize the textual information, despite the fact that all documents in the log are unique.
However, the choice of text model has a minimal influence on performance for all prefix lengths.

On the werk.nl log the next activity prediction accuracy is about 1\% 


\pgfplotscreateplotcyclelist{colorlist}{%
	cyan!60!black,every mark/.append style={fill=cyan!60!black},mark=*\\%1
	orange!60!black,every mark/.append style={fill=orange!60!black},mark=square*\\%2
	darkgray!60!black,every mark/.append style={fill=darkgray!60!black},mark=otimes*\\%3
	red!60!black,every mark/.append style={fill=red!60!black},mark=triangle*\\%4
	olive!60!black,every mark/.append style={fill=olive!60!black},mark=diamond*\\%5
}

\begin{figure}[!htbp]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\begin{tikzpicture}[scale=0.9]
			\begin{axis}[
				xlabel={Prefix Length},
				ylabel={Accuracy},
				legend pos=north west,
				cycle list name=colorlist,
				]
				\addplot table[x=index,y=na-0, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{Baseline}
				\addplot table[x=index,y=naBoW100, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{BoW}
				\addplot table[x=index,y=naBoNG100, col sep= comma]  {data/prefix_application.csv};
				\addlegendentry{BoNG}
				\addplot table[x=index,y=naPV100, col sep= comma]  {data/prefix_application.csv};
				\addlegendentry{PV}
				\addplot table[x=index,y=naLDA100, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{LDA}
				\legend{}
			\end{axis}
		\end{tikzpicture}%
		\hfill
		\begin{tikzpicture}[scale=0.9]
			\begin{axis}[
				xlabel={Prefix Length},
				ylabel={Mean Absolute Error},
				legend pos=north west,
				cycle list name=colorlist,
				]
				\addplot table[x=index,y=nt-0, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{Baseline}
				\addplot table[x=index,y=ntBoW500, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{BoW}
				\addplot table[x=index,y=ntBoNG100, col sep= comma]  {data/prefix_application.csv};
				\addlegendentry{BoNG}
				\addplot table[x=index,y=ntPV100, col sep= comma]  {data/prefix_application.csv};
				\addlegendentry{PV}
				\addplot table[x=index,y=ntLDA10, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{LDA}
				\legend{}
			\end{axis}
		\end{tikzpicture}
	\caption{Next activity accuracy and timestamp MAE on Job Application data set.}
	\vspace{0.5cm}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
	\centering
	\begin{tikzpicture}[scale=0.9]
		\begin{axis}[
			xlabel={Prefix Length},
			ylabel={Accuracy},
			legend pos=north west,
			cycle list name=colorlist,
			]
			\addplot table[x=index,y=na-0, col sep= comma] {data/prefix_werk.csv};
			\addlegendentry{Baseline}
			\addplot table[x=index,y=naBoW100, col sep= comma] {data/prefix_werk.csv};
			\addlegendentry{BoW}
			\addplot table[x=index,y=naBoNG500, col sep= comma] {data/prefix_werk.csv};
			\addlegendentry{BoNG}
			\addplot table[x=index,y=naPV20, col sep= comma]  {data/prefix_werk.csv};
			\addlegendentry{PV}
			\addplot table[x=index,y=naLDA100, col sep= comma] {data/prefix_werk.csv};
			\addlegendentry{LDA}
			\legend{}
		\end{axis}
	\end{tikzpicture}%
	\hfill
	\begin{tikzpicture}[scale=0.9]
		\begin{axis}[
			xlabel={Prefix Length},
			ylabel={Mean Absolute Error},
			legend pos=north east,
			cycle list name=colorlist,
			]
			\addplot table[x=index,y=nt-0, col sep= comma] {data/prefix_werk.csv};
			\addlegendentry{Baseline}
			\addplot table[x=index,y=ntBoW100, col sep= comma] {data/prefix_werk.csv};
			\addlegendentry{BoW}
			\addplot table[x=index,y=ntBoNG500, col sep= comma] {data/prefix_werk.csv};
			\addlegendentry{BoNG}
			\addplot table[x=index,y=ntPV100, col sep= comma]  {data/prefix_werk.csv};
			\addlegendentry{PV}
			\addplot table[x=index,y=ntLDA20, col sep= comma] {data/prefix_werk.csv};
			\addlegendentry{LDA}
		\end{axis}
	\end{tikzpicture}
	\caption{Next activity accuracy and timestamp MAE on werk.nl data set.}
\end{subfigure}
	\caption[Next activity and timestamp prediction performance subject to the prefix length]{Next activity and timestamp prediction performance subject to the prefix length.}
	\label{fig:next-activity-prefix}
\end{figure}



\section{Outcome and Case Cycle Time Prediction}

\begin{table}[!htbp]
	\newrobustcmd{\B}{\fontseries{b}\selectfont}
	\setlength\tabcolsep{3pt}
	\begin{tabularx}{\textwidth}{
			>{\hsize=1.0\hsize}X
			>{\hsize=1.0\hsize}X
			>{\hsize=1.0\hsize}X
			>{\hsize=1.0\hsize}X
			>{\hsize=1.0\hsize}X
			>{\hsize=1.0\hsize}X
			>{\hsize=1.0\hsize}X
			>{\hsize=1.0\hsize}X
		}
		\toprule
		& & \multicolumn{2}{l}{\textbf{Job Application}} & \multicolumn{2}{l}{\textbf{werk.nl}} & \multicolumn{2}{l}{\textbf{Help Desk}} \\
		Text & Text &Outcome & Cycle & Outcome& Cycle  & Outcome& Cycle  \\
		Model & Dimension &Accuracy & MAE & Accuracy& MAE  & Accuracy& MAE  \\
		\midrule
		\multicolumn{8}{c}{\textit{Text-Aware Process Prediction (LSTM)}} \\
		BoW&50&   \B   0.7139&     1.1081&     0.5346&     0.2073\\
		BoW&100&   \B   0.7139&     1.1083&     0.5411&  \B    0.1925\\
		BoW&500&   \B   0.7139&     1.0698&     0.5276&     0.2027\\
		BoNG&50&     0.7138&     1.0922&     0.5492&     0.1987\\
		BoNG&100&     0.7137&     1.0122&     0.5378&     0.2075\\
		BoNG&500&     0.6948&  \B   0.9918&     0.5241&     0.1930\\
		PV&10&     0.6639&     1.4796&     0.5464&     0.2163\\
		PV&20&     0.7009&     1.2325&     0.5545&     0.2047\\
		PV&100&     0.7130&     1.0390&     0.5378&     0.2092\\
		LDA&10&   \B   0.7139&     1.1083&     0.5508&     0.2091\\
		LDA&20&    \B  0.7139&     1.1178&     0.5531&     0.2033\\
		LDA&100&   \B   0.7139&     1.1134&     0.5442&     0.2029\\
		\multicolumn{8}{c}{\textit{LSTM baseline}} \\
		-&0&     0.6065&     1.6523&  \B    0.5593&     0.1959\\
		\bottomrule
	\end{tabularx}
	\caption[Experimental results for the outcome and cycle time prediction]{Experimental results for the outcome and cycle time prediction.}
	\label{tab:outcome-cycle-time}
\end{table}

The case outcome and cycle time prediction is evaluated analogously to next event prediction.
In contrast to the next event prediction, the target values are case properties that are the same for every prefix trace of the case.
As outcome the final activity of the trace is used as described in Section \ref{sec:overview}.
The results are summarized in in Table \ref{tab:outcome-cycle-time}.

The prediction performance of the text-aware models is significantly higher for the job application event log for both prediction tasks, but not for the werk.nl log.
On the job application
Regarding the outcome prediction for the werk.nl log the baseline LSTM approach achieves the highest accuracy.








\begin{figure}[!htbp]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\begin{tikzpicture}[scale=0.9]
			\begin{axis}[
				xlabel={Prefix Length},
				ylabel={Accuracy},
				legend pos=north west,
				cycle list name=colorlist,
				]
				\addplot table[x=index,y=oa-0, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{Baseline}
				\addplot table[x=index,y=oaBoW100, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{BoW}
				\addplot table[x=index,y=oaBoNG50, col sep= comma]  {data/prefix_application.csv};
				\addlegendentry{BoNG}
				\addplot table[x=index,y=oaPV100, col sep= comma]  {data/prefix_application.csv};
				\addlegendentry{PV}
				\addplot table[x=index,y=oaLDA100, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{LDA}
				\legend{}
			\end{axis}
		\end{tikzpicture}%
		\hfill
		\begin{tikzpicture}[scale=0.9]
			\begin{axis}[
				xlabel={Prefix Length},
				ylabel={Mean Absolute Error},
				legend pos=north west,
				cycle list name=colorlist,
				]
				\addplot table[x=index,y=ct-0, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{Baseline}
				\addplot table[x=index,y=ctBoW500, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{BoW}
				\addplot table[x=index,y=ctBoNG500, col sep= comma]  {data/prefix_application.csv};
				\addlegendentry{BoNG}
				\addplot table[x=index,y=ctPV100, col sep= comma]  {data/prefix_application.csv};
				\addlegendentry{PV}
				\addplot table[x=index,y=ctLDA10, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{LDA}
				\legend{}
			\end{axis}
		\end{tikzpicture}
		\caption{Case outcome accuracy and case cycle time MAE on Job Application data set.}
		\vspace{0.5cm}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\begin{tikzpicture}[scale=0.9]
			\begin{axis}[
				xlabel={Prefix Length},
				ylabel={Accuracy},
				legend pos=north west,
				cycle list name=colorlist,
				]
				\addplot table[x=index,y=oa-0, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{Baseline}
				\addplot table[x=index,y=oaBoW100, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{BoW}
				\addplot table[x=index,y=oaBoNG50, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{BoNG}
				\addplot table[x=index,y=oaPV20, col sep= comma]  {data/prefix_werk.csv};
				\addlegendentry{PV}
				\addplot table[x=index,y=oaLDA20, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{LDA}
				\legend{}
			\end{axis}
		\end{tikzpicture}%
		\hfill
		\begin{tikzpicture}[scale=0.9]
			\begin{axis}[
				xlabel={Prefix Length},
				ylabel={Mean Absolute Error},
				legend pos=north east,
				cycle list name=colorlist,
				]
				\addplot table[x=index,y=ct-0, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{Baseline}
				\addplot table[x=index,y=ctBoW100, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{BoW}
				\addplot table[x=index,y=ctBoNG500, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{BoNG}
				\addplot table[x=index,y=ctPV20, col sep= comma]  {data/prefix_werk.csv};
				\addlegendentry{PV}
				\addplot table[x=index,y=ctLDA100, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{LDA}
			\end{axis}
		\end{tikzpicture}
	\caption{Case outcome accuracy and case cycle time MAE on werk.nl data set.}
	\end{subfigure}
	\caption[Case outcome and cycle time prediction performance subject to the prefix length]{Case outcome and cycle time prediction performance subject to the prefix length.}
	\label{fig:outcome-cycle-time-prefix}
\end{figure}

