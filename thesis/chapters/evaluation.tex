In this chapter, the performance of the text-aware process prediction model is evaluated based on simulated and real-world event data.
First, the data sets and evaluation method are described. Then, the performance of differently parameterized models on the data sets is evaluated and analyzed in-depth.


\section{Data Sets}

The prediction model is evaluated on one simulated and two real-world event logs, which are described in the following.
An overview of the key properties of the logs is summarized in Table \ref{tab:logs}.

\textbf{Job Application} (Simulated) This event log describes a simple job application process. 
First, the applicant starts an application in the company's system.
Then, he or she uploads a curriculum vitae (CV) and optionally a cover letter in an random order.
When the documents have been received, the applicant is either directly rejected by the company or invited to an interview.
In case of an invitation the applicant responds in an email, if he or she would like to accept or reject the invitation.
After the interview a decision is made and sent to the applicant that states, if the applicant gets a job offer or is rejected.
in case of a job offer, the applicant answers in an email if he or she would like to accept the job offer.
Furthermore, noise is added by introducing a 1\% probability after each event that the process stops immediately.

The timestamp of each event is determined by sampling from a normal distribution, which mean and variance is unique per activity.
The CV, cover letter and all emails are available as a text attribute in the event log.
The text information is generated by sampling 10 times from sets of full and partial sentences depending on the control flow
For example, if an applicant gets a job offer, the generated email contains text fragments from typical job offer emails.
If on the other hand the applicant is rejected, the email is generated with sentences from typical rejection emails.
With this text generation mechanism, all texts in the event log are unique, but the words and sentences in the texts correlate with the path of the corresponding case.

\textbf{Customer Journey} (Real-world) This event log describes customer's journeys of the Employee Insurance Agency commissioned by the Dutch Ministry of Social Affairs and Employment.
The log is aggregated from two anonymized data sets, that were provided in the BPI Challenge 2016 \cite{bpichallenge2016}, containing click data of customers logged in the official website werk.nl and phone call data from the call center.
Both data sets are joined based on the customer ID to derived a detailed view on customer contacts in the web and on the phone.
For each phone call the costumer's question is available as a text attribute in English.
In addition, the customer's age group and gender is considered as additional attributes.
The event log is filtered to remove outlier activities (threshold <0.5\%) and infrequent trace variants (2 or less traces with the same variant).
 
\textbf{Hospital Admission} (Real-world) This log is generated from the MIMIC-III (Medical Information Mart for Intensive Care) database \cite{johnson2016mimic} and contains hospital admission and discharge events of patients in the Beth Israel Deaconess Medical Center between 2001 and 2012.
Next to the admission and discharge locations that define the activity, the admission type (e.g. emergency) and insurance (e.g. private) of the patient is considered as additional attributes.
Furthermore, each admission event contains a diagnosis as text information.
A case contains all the admission and discharge events of a single patient.
Admission and discharge events occur alternating and every admission event is followed by a discharge event.

The logs represent three different levels of process complexity.
The job application log contains 11 different activities, 41 trace variants and 185 unique words (after preprocessing) in the text attribute.
Therefore, this log can be considered as quite structured and is subsequently easier to predict.
In contrast, the customer journey log has 18 different activities, 1\,001 trace variants and 817 unique words.
The hospital admission log is the most complex with 26 activities, 2\,784 trace variants and 4\,633 (!) unique words.

\begin{table}[!htbp]
	\begin{tabularx}{\textwidth}{l r r r}
		\toprule
		\textbf{Event Log} & \textbf{Job} & \textbf{Customer} &\textbf{Hospital}  \\
		 & \textbf{Application} & \textbf{Journey} &\textbf{Admission}  \\
		\midrule
		Cases & 20\,000& 15\,001& 46\,520\\
		Trace variants &41 & 1\,001 &2\,784 \\
		Events & 118\,811 & 55\,220 & 117\,952\\
		Events per case (mean) & 5.941& 3.681& 2.536\\
		Median case duration (days) & 1.9876 & 0.224& 7.579\\
		Mean case duration (days)& 3.1524 &  0.713 & 121.154\\
		Activities & 11 & 18 & 26\\
		Words before preprocessing & 3\,050\,594 &247\,010 &  171\,938\\
		Words after preprocessing  &1\,519\,199 &98\,915 & 165\,285\\
		Vocabulary before preprocessing & 237 & 1\,203 & 4\,973 \\
		Vocabulary after preprocessing & 185 & 817 & 4\,633\\
		Text attribute & email& customer question & diagnosis\\
		Additional non-textual attributes & - & gender& admission type\\
		 &  & age& insurance\\
		\bottomrule
	\end{tabularx}
	\caption[Overview of evaluated data sets]{Overview of evaluated data sets.}
	\label{tab:logs}
\end{table}

\section{Evaluation Method}

Each event log is evaluated in a consistent procedure.
In the first step, the event log is separated into a training and test log. 
The training log consists of the first 2/3 chronologically ordered traces and is used to fit the prediction model to the historical data.
The remaining 1/3 of traces are used to measure the prediction performance.
For each trace $\sigma$ in the test log, all prefixes $hd^k(\sigma)$ of length $1 \leq k \leq |\sigma| - 1$ are considered as instances for prediction.

The LSTM network is trained with at most 25 epochs and the learning rate is initialized with 0.001.
During the training of the LSTM model, 20\% of the training log is used for validation: If the error on the validation log is not decreasing anymore, the training rate is reduced by a factor of 10.
In addition, if the error is not decreasing for 3 epochs in a row, the training is stopped in order to avoid overfitting.
Furthermore, the LSTM layers use dropout \cite{DBLP:journals/corr/abs-1207-0580} of 20\% during training as an additional measure against overfitting.

For classification (i.e. categorical prediction) task, like next event and outcome prediction, the accuracy is utilized as metric.
The accuracy is computed as the number of correct predictions $t$ divided by the total number of predictions $n$, i.e. 
\begin{equation*}
	\textrm{accuracy} = \dfrac{t}{n} = \dfrac{\textrm{\# correct predictions}}{\textrm{\# total predictions}} \in [0,1].
\end{equation*}

For regression tasks, like the next event time and the case cycle time predictions, the mean absolute error (MAE) is computed to measure the prediction performance. The mean absolute error indicates the average absolute difference between the predicted value $\hat{y}$ and the true value $y$,  precisely
\begin{equation*}
	\textrm{MAE} = \dfrac{1}{n}\sum_{i=1}^{n}|\hat{y_i} - y_i| \in [0, \infty).
\end{equation*}
This error metric is favored, since it gives a more intuitive interpretation and is less sensitive to outliers compared to similar metrics like the mean squared error (MSE).
An accuracy of 1 and a MAE of 0 is the most desirable.

The text-aware process prediction model is evaluated with all presented text models namely Bag of Words, Bag of N-Gram, Paragraph Vector and Latent Dirichlet Allocation.
Each model is tested with 3 different encoding lengths for the text.
The BoW and BoNG models are evaluated with 50, 100 and 500 dimensional text vectors, the PV and LDA models with smaller text encodings of size 10, 20 and 100.
For the Bag of Words and Bag of N-Gram model the encoding length is adjusted by only considering the most frequent terms in the vocabulary.
The encoding dimension of the non-textual data depends on the considered attributes and their number of the distinct values in the event logs.
The Bag of N-Gram model is used with bigrams ($n=2$) and the Paragraph Vector model is trained for 15 epochs.

The text-aware prediction model is compared to the pure LSTM model approach based on the ideas of \cite{DBLP:conf/caise/TaxVRD17} and \cite{DBLP:conf/ssci/NavarinVPS17}, that only considers the activity, timestamp and additional non-textual data.
The latter can be considered as the current state-of-the-art approach in process prediction, if the prediction performance is the only criteria.


\section{Next Activity and Timestamp Prediction}

\begin{table}[!htbp]
	\newrobustcmd{\B}{\fontseries{b}\selectfont}
	\setlength\tabcolsep{3pt}
	\begin{tabularx}{\textwidth}{
			>{\hsize=0.8\hsize}C
			>{\hsize=1.2\hsize}C
			>{\hsize=1.0\hsize}C
			>{\hsize=1.0\hsize}C
			>{\hsize=1.0\hsize}C
			>{\hsize=1.0\hsize}C
			>{\hsize=1.0\hsize}C
			>{\hsize=1.0\hsize}C
		}
		\toprule
		& & \multicolumn{2}{l}{\textbf{Job Application}} & \multicolumn{2}{l}{\textbf{Customer Journey}} & \multicolumn{2}{l}{\textbf{Hospital Admission}} \\
		Text & Text &Activity & Time & Activity& Time  & Activity& Time  \\
		Model & Dimension &Accuracy & MAE & Accuracy& MAE  & Accuracy& MAE  \\
		\midrule
		\multicolumn{8}{c}{\textit{Text-Aware Process Prediction (LSTM)}} \\
BoW&50&     0.8892&     0.1043&     0.4896&     0.1787&     0.5982&    27.9673\\
BoW&100& \B    0.8894&     0.1046& \B    0.4906&  \B   0.1767&     0.6045&    27.8268\\
BoW&500&     0.8890&     0.1067&     0.4842&     0.1796&  \B   0.6134&    30.7915\\
BoNG&50&     0.8734&     0.1315&     0.4889&  \B   0.1767&     0.5916&    27.6287\\
BoNG&100&     0.8870&     0.1087&     0.4875&     0.1783&     0.6010&    27.9706\\
BoNG&500&     0.8888&  \B   0.1040&     0.4867&     0.1829&     0.6020&    28.3840\\
PV&10&     0.8316&     0.1749&     0.4802&     0.1789&     0.5877&    28.9150\\
PV&20&     0.8760&     0.1180&     0.4825&     0.1781&     0.5889&    27.6080\\
PV&100&     0.8869&     0.1194&     0.4825&     0.1777&     0.5896&    27.4488\\
LDA&10&     0.8754&     0.1370&     0.4833&     0.1783&     0.5859&    27.7591\\
LDA&20&     0.8892&     0.1045&     0.4886&     0.1780&     0.5920&    27.9961\\
LDA&100&     0.8892&     0.1045&     0.4878&     0.1772&     0.6027&    28.0485\\
		\multicolumn{8}{c}{\textit{LSTM baseline}} \\
\multicolumn{2}{l}{Based on \cite{DBLP:conf/caise/TaxVRD17}+\cite{DBLP:conf/ssci/NavarinVPS17}} &     0.7647&     0.2502&     0.4740&     0.1779&     0.5870&  \B  27.3528\\
		\bottomrule
	\end{tabularx}
	\caption[Experimental results for the next event prediction]{Experimental results for the next event prediction.}
	\label{tab:next-event}
\end{table}

First, the prediction performance regarding the activity and timestamp of the next event is evaluated.
The results are summarized in Table \ref{tab:next-event}.
Each line in the table states the prediction accuracy for the next activity and the mean absolute error in days for the next timestamp prediction of a single model on all evaluated event logs.

Compared to the LSTM baseline, the text-aware model is able to improve the prediction performance for the next activity and timestamp on all data sets for at least one 
parameterization.
However, the effect of textual data is varies a lot between the data sets and the prediction tasks.
A reason for that is, that the textual data in the simulated log correlates with the control flow by design, where in the real-world data the correlation is only assumed and probably significantly lower.

\pgfplotscreateplotcyclelist{colorlist}{%
	cyan!60!black,every mark/.append style={fill=cyan!60!black},mark=*\\%1
	orange!60!black,every mark/.append style={fill=orange!60!black},mark=square*\\%2
	darkgray!60!black,every mark/.append style={fill=darkgray!60!black},mark=otimes*\\%3
	red!60!black,every mark/.append style={fill=red!60!black},mark=triangle*\\%4
	olive!60!black,every mark/.append style={fill=olive!60!black},mark=diamond*\\%5
}

\begin{figure}[!htbp]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\begin{tikzpicture}[scale=0.9]
			\begin{axis}[
				xlabel={Prefix Length},
				ylabel={Accuracy},
				legend pos=north west,
				cycle list name=colorlist,
				]
				\addplot table[x=index,y=na-0, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{Baseline}
				\addplot table[x=index,y=naBoW50, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{BoW}
				\addplot table[x=index,y=naBoNG100, col sep= comma]  {data/prefix_application.csv};
				\addlegendentry{BoNG}
				\addplot table[x=index,y=naPV100, col sep= comma]  {data/prefix_application.csv};
				\addlegendentry{PV}
				\addplot table[x=index,y=naLDA20, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{LDA}
				\legend{}
			\end{axis}
		\end{tikzpicture}%
		\hfill
		\begin{tikzpicture}[scale=0.9]
			\begin{axis}[
				xlabel={Prefix Length},
				ylabel={Mean Absolute Error},
				legend pos=north west,
				cycle list name=colorlist,
				]
				\addplot table[x=index,y=nt-0, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{Baseline}
				\addplot table[x=index,y=ntBoW500, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{BoW}
				\addplot table[x=index,y=ntBoNG500, col sep= comma]  {data/prefix_application.csv};
				\addlegendentry{BoNG}
				\addplot table[x=index,y=ntPV20, col sep= comma]  {data/prefix_application.csv};
				\addlegendentry{PV}
				\addplot table[x=index,y=ntLDA10, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{LDA}
				\legend{}
			\end{axis}
		\end{tikzpicture}
		\caption{Next activity accuracy and timestamp MAE on Job Application data set.}
		\vspace{0.5cm}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\begin{tikzpicture}[scale=0.9]
			\begin{axis}[
				xlabel={Prefix Length},
				ylabel={Accuracy},
				legend pos=north west,
				cycle list name=colorlist,
				]
				\addplot table[x=index,y=na-0, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{Baseline}
				\addplot table[x=index,y=naBoW50, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{BoW}
				\addplot table[x=index,y=naBoNG100, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{BoNG}
				\addplot table[x=index,y=naPV100, col sep= comma]  {data/prefix_werk.csv};
				\addlegendentry{PV}
				\addplot table[x=index,y=naLDA100, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{LDA}
				\legend{}
			\end{axis}
		\end{tikzpicture}%
		\hfill
		\begin{tikzpicture}[scale=0.9]
			\begin{axis}[
				xlabel={Prefix Length},
				ylabel={Mean Absolute Error},
				legend pos=north east,
				cycle list name=colorlist,
				]
				\addplot table[x=index,y=nt-0, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{Baseline}
				\addplot table[x=index,y=ntBoW50, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{BoW}
				\addplot table[x=index,y=ntBoNG100, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{BoNG}
				\addplot table[x=index,y=ntPV100, col sep= comma]  {data/prefix_werk.csv};
				\addlegendentry{PV}
				\addplot table[x=index,y=ntLDA10, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{LDA}
			\end{axis}
		\end{tikzpicture}
		\caption{Next activity accuracy and timestamp MAE on werk.nl data set.}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\begin{tikzpicture}[scale=0.9]
			\begin{axis}[
				xlabel={Prefix Length},
				ylabel={Accuracy},
				legend pos=north west,
				cycle list name=colorlist,
				]
				\addplot table[x=index,y=na-0, col sep= comma] {data/prefix_admissions.csv};
				\addlegendentry{Baseline}
				\addplot table[x=index,y=naBoW500, col sep= comma] {data/prefix_admissions.csv};
				\addlegendentry{BoW}
				\addplot table[x=index,y=naBoNG100, col sep= comma] {data/prefix_admissions.csv};
				\addlegendentry{BoNG}
				\addplot table[x=index,y=naPV100, col sep= comma]  {data/prefix_admissions.csv};
				\addlegendentry{PV}
				\addplot table[x=index,y=naLDA100, col sep= comma] {data/prefix_admissions.csv};
				\addlegendentry{LDA}
				\legend{}
			\end{axis}
		\end{tikzpicture}%
		\hfill
		\begin{tikzpicture}[scale=0.9]
			\begin{axis}[
				xlabel={Prefix Length},
				ylabel={Mean Absolute Error},
				legend pos=north east,
				cycle list name=colorlist,
				]
				\addplot table[x=index,y=nt-0, col sep= comma] {data/prefix_admissions.csv};
				\addlegendentry{Baseline}
				\addplot table[x=index,y=ntBoW500, col sep= comma] {data/prefix_admissions.csv};
				\addlegendentry{BoW}
				\addplot table[x=index,y=ntBoNG50, col sep= comma] {data/prefix_admissions.csv};
				\addlegendentry{BoNG}
				\addplot table[x=index,y=ntPV20, col sep= comma]  {data/prefix_admissions.csv};
				\addlegendentry{PV}
				\addplot table[x=index,y=ntLDA10, col sep= comma] {data/prefix_admissions.csv};
				\addlegendentry{LDA}
				\legend{}
			\end{axis}
		\end{tikzpicture}
		\caption{Next activity accuracy and timestamp MAE on werk.nl data set.}
	\end{subfigure}
	\caption[Next activity and timestamp prediction performance subject to the prefix length]{Next activity and timestamp prediction performance subject to the prefix length.}
	\label{fig:next-activity-prefix}
\end{figure}

On the job application log the accuracy of the next activity prediction is increased by up to 14.95 percentage points using the textual data.
In contrast, on the customer journey log the prediction is improved by at most 1.82 percentage points.
On the hospital admission log the prediction accuracy is improved by at most 0.031 percentage points and even reduced for some models.
The hospital log has the biggest vocabulary by far with 4\,633 unique words and contains 2\,784 trace variants.
Therefore, the utilization of textual data has a rather small impact.

The timestamp prediction is significantly improved on the job application log (0.1232 vs 0.2995 MAE), but only a little bit improved on the customer journey log.
On the hospital admission log the text-aware approach makes worse timestamp prediction on the majority of models.

The choice of text model has a rather small impact on the results.
Notably, the BoW and LDA model perform most consistently on all data sets and all text encoding lengths.
The PV model reaches similar results using a hundred dimensional text vector, but performs worse when a small text embedding size of 10 or 20 is used.
The BoNG model has a similar performance compared to the BoW model and delivers the best results on the customer journey log.
Nevertheless, the word order awareness of the BoNG model does not necessarily convert to better prediction results on others logs.

In addition, the prediction performance is evaluated per prefix length for each event log.
Figure \ref{fig:next-activity-prefix} shows the next activity prediction accuracy and next timestamp MAE for every prefix trace of length $1 \leq k \leq 8$.
For the text-aware models only the best text encoding length is shown in the diagram.
On the job application event log all text-aware process models almost perfectly predict the next activity after 3 events (i.e. after the application documents are available).
This shows that all text-aware approaches can unequivocally take advantage of the emails at every decision point in the process, despite the fact that all documents in the log are unique.
The improvements on the activity prediction lead consequently to significantly better timestamp predictions on this log.



\section{Outcome and Case Cycle Time Prediction}

\begin{table}[!htbp]
	\newrobustcmd{\B}{\fontseries{b}\selectfont}
	\setlength\tabcolsep{3pt}
	\begin{tabularx}{\textwidth}{
			>{\hsize=0.8\hsize}C
			>{\hsize=1.2\hsize}C
			>{\hsize=1.0\hsize}C
			>{\hsize=1.0\hsize}C
			>{\hsize=1.0\hsize}C
			>{\hsize=1.0\hsize}C
			>{\hsize=1.0\hsize}C
			>{\hsize=1.0\hsize}C
		}
		\toprule
		& & \multicolumn{2}{c}{\textbf{Job Application}} & \multicolumn{2}{c}{\textbf{Customer Journey}} & \multicolumn{2}{c}{\textbf{Hospital Admission}} \\
		Text & Text &Outcome & Cycle & Outcome& Cycle  & Outcome& Cycle  \\
		Model & Dimension &Accuracy & MAE & Accuracy& MAE  & Accuracy& MAE  \\
		\midrule
		\multicolumn{8}{c}{\textit{Text-Aware Process Prediction (LSTM)}} \\
BoW&50&  \B   0.6762&     1.4150&     0.5095&     0.3144&     0.6136&    94.8308\\
BoW&100&     0.6687&     1.4203&     0.5011&     0.3136&     0.6190&    97.2484\\
BoW&500&     0.6757&     1.4146&     0.4984&     0.3090&  \B   0.6198&   100.8792\\
BoNG&50&     0.6632&     1.4495&     0.5115& \B    0.2995&     0.6149&    95.8228\\
BoNG&100&     0.6746&     1.4237&     0.5040&     0.3105&     0.6151&    91.5891\\
BoNG&500&     0.6694&     1.4235&     0.4968&     0.3068&     0.6081&    96.1264\\
PV&10&     0.6253&     1.5803&     0.5123&     0.3173&     0.6050&    92.6440\\
PV&20&     0.6644&     1.4951&     0.5106&     0.3141&     0.6057&    92.7924\\
PV&100&     0.6678&     1.4577&     0.5083&     0.3167&     0.6103&    93.0984\\
LDA&10&     0.6684&     1.4541&     0.5099&     0.3225&     0.6065&    92.6729\\
LDA&20&     0.6750&   \B  1.4132&  \B   0.5165&     0.3129&     0.6057&   101.0274\\
LDA&100&     0.6744&     1.4183&     0.5114&     0.3091&     0.6125& \B   91.1070\\
		\multicolumn{8}{c}{\textit{LSTM baseline}} \\
\multicolumn{2}{l}{Based on \cite{DBLP:conf/caise/TaxVRD17}+\cite{DBLP:conf/ssci/NavarinVPS17}} &  0.5836&     1.7144&     0.5083&     0.3158&     0.6043&    91.6329 \\
		\bottomrule
	\end{tabularx}
	\caption[Experimental results for the outcome and cycle time prediction]{Experimental results for the outcome and cycle time prediction.}
	\label{tab:outcome-cycle-time}
\end{table}

The case outcome and cycle time prediction is evaluated analogously to next event prediction.
In contrast to the next event prediction, the target values are case properties, i.e. they are the same for every prefix trace of the case.
As outcome the final activity of the trace is used as described in Section \ref{sec:overview}.
The results are summarized in Table \ref{tab:outcome-cycle-time}.


\begin{figure}[!htbp]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\begin{tikzpicture}[scale=0.9]
			\begin{axis}[
				xlabel={Prefix Length},
				ylabel={Accuracy},
				legend pos=north west,
				cycle list name=colorlist,
				]
				\addplot table[x=index,y=oa-0, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{Baseline}
				\addplot table[x=index,y=oaBoW500, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{BoW}
				\addplot table[x=index,y=oaBoNG100, col sep= comma]  {data/prefix_application.csv};
				\addlegendentry{BoNG}
				\addplot table[x=index,y=oaPV100, col sep= comma]  {data/prefix_application.csv};
				\addlegendentry{PV}
				\addplot table[x=index,y=oaLDA20, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{LDA}
				\legend{}
			\end{axis}
		\end{tikzpicture}%
		\hfill
		\begin{tikzpicture}[scale=0.9]
			\begin{axis}[
				xlabel={Prefix Length},
				ylabel={Mean Absolute Error},
				legend pos=north west,
				cycle list name=colorlist,
				]
				\addplot table[x=index,y=ct-0, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{Baseline}
				\addplot table[x=index,y=ctBoW500, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{BoW}
				\addplot table[x=index,y=ctBoNG500, col sep= comma]  {data/prefix_application.csv};
				\addlegendentry{BoNG}
				\addplot table[x=index,y=ctPV100, col sep= comma]  {data/prefix_application.csv};
				\addlegendentry{PV}
				\addplot table[x=index,y=ctLDA20, col sep= comma] {data/prefix_application.csv};
				\addlegendentry{LDA}
				\legend{}
			\end{axis}
		\end{tikzpicture}
		\caption{Case outcome accuracy and case cycle time MAE on Job Application data set.}
		\vspace{0.5cm}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\begin{tikzpicture}[scale=0.9]
			\begin{axis}[
				xlabel={Prefix Length},
				ylabel={Accuracy},
				legend pos=north west,
				cycle list name=colorlist,
				]
				\addplot table[x=index,y=oa-0, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{Baseline}
				\addplot table[x=index,y=oaBoW50, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{BoW}
				\addplot table[x=index,y=oaBoNG50, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{BoNG}
				\addplot table[x=index,y=oaPV10, col sep= comma]  {data/prefix_werk.csv};
				\addlegendentry{PV}
				\addplot table[x=index,y=oaLDA20, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{LDA}
				\legend{}
			\end{axis}
		\end{tikzpicture}%
		\hfill
		\begin{tikzpicture}[scale=0.9]
			\begin{axis}[
				xlabel={Prefix Length},
				ylabel={Mean Absolute Error},
				legend pos=north east,
				cycle list name=colorlist,
				]
				\addplot table[x=index,y=ct-0, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{Baseline}
				\addplot table[x=index,y=ctBoW500, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{BoW}
				\addplot table[x=index,y=ctBoNG50, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{BoNG}
				\addplot table[x=index,y=ctPV20, col sep= comma]  {data/prefix_werk.csv};
				\addlegendentry{PV}
				\addplot table[x=index,y=ctLDA100, col sep= comma] {data/prefix_werk.csv};
				\addlegendentry{LDA}
				
			\end{axis}
		\end{tikzpicture}
	\caption{Case outcome accuracy and case cycle time MAE on werk.nl data set.}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
	\centering
	\begin{tikzpicture}[scale=0.9]
		\begin{axis}[
			xlabel={Prefix Length},
			ylabel={Accuracy},
			legend pos=north west,
			cycle list name=colorlist,
			]
			\addplot table[x=index,y=oa-0, col sep= comma] {data/prefix_admissions.csv};
			\addlegendentry{Baseline}
			\addplot table[x=index,y=oaBoW500, col sep= comma] {data/prefix_admissions.csv};
			\addlegendentry{BoW}
			\addplot table[x=index,y=oaBoNG500, col sep= comma] {data/prefix_admissions.csv};
			\addlegendentry{BoNG}
			\addplot table[x=index,y=oaPV100, col sep= comma]  {data/prefix_admissions.csv};
			\addlegendentry{PV}
			\addplot table[x=index,y=oaLDA100, col sep= comma] {data/prefix_admissions.csv};
			\addlegendentry{LDA}
			\legend{}
		\end{axis}
	\end{tikzpicture}%
	\hfill
	\begin{tikzpicture}[scale=0.9]
		\begin{axis}[
			xlabel={Prefix Length},
			ylabel={Mean Absolute Error},
			legend pos=north east,
			cycle list name=colorlist,
			]
			\addplot table[x=index,y=ct-0, col sep= comma] {data/prefix_admissions.csv};
			\addlegendentry{Baseline}
			\addplot table[x=index,y=ctBoW100, col sep= comma] {data/prefix_admissions.csv};
			\addlegendentry{BoW}
			\addplot table[x=index,y=ctBoNG50, col sep= comma] {data/prefix_admissions.csv};
			\addlegendentry{BoNG}
			\addplot table[x=index,y=ctPV100, col sep= comma]  {data/prefix_admissions.csv};
			\addlegendentry{PV}
			\addplot table[x=index,y=ctLDA10, col sep= comma] {data/prefix_admissions.csv};
			\addlegendentry{LDA}
			\legend{}
		\end{axis}
	\end{tikzpicture}
	\caption{Case outcome accuracy and case cycle time MAE on werk.nl data set.}
\end{subfigure}
	\caption[Case outcome and cycle time prediction performance subject to the prefix length]{Case outcome and cycle time prediction performance subject to the prefix length.}
	\label{fig:outcome-cycle-time-prefix}
\end{figure}

On the job application log the text-aware prediction model can predict the outcome of the case with 67.62\% accuracy using the BoW text model compared to 58.36\% accuracy on the baseline approach.
The MAE on the case cycle time prediction can be reduced to 1.4132 compared to 1.7144 on the baseline approach.

On the customer journey log the text-awareness leads to much smaller improvements.
The best outcome prediction is realized with the LDA text model (51.65\% accuracy), which improves the prediction by 0.83 percentage points.
The case cycle time MAE is the lowest using the BoNG model with 0.2995, which improves the baseline prediction by 0.0163 days (= 23.47 minutes).
Strikingly, some text-aware models have slightly worse predictions like the BoW and BoNG models with 500 dimensional text encodings on the outcome prediction tasks.
A possible reasons for that is that with a vocabulary size of 817 after preprocessing also infrequent words are encoded, which provide rather low prediction value.

On the hospital admission log the text-aware approach delivers improvements on the outcome prediction task by up to 1.55 percentage points using the BoW model.
However, the cycle time prediction is worse for almost all text-aware models except the LDA and BoNG model with 100 dimensional text encodings.
As observed during the next timestamp prediction, the text-aware models have difficulties to improve on time-related regression task on this particular log.

Again, the choice of text model has a rather small impact.
The BoW and LDA models perform slightly better than the BoNG and PV model.
Notably, the PV model can not achieve the best performance on any log and  prediction task.

\section{Discussion and Key Findings}

The text-aware process prediction model outperforms the baseline approach on all almost all evaluated event logs and prediction tasks.
The influence of textual data on the prediction performance varies greatly per event log.
It is observed that the impact of the text consideration on the classification tasks is significantly higher compared to the timestamp regression tasks.
All evaluated text models are suitable to capture textual data and deliver similar results on most prediction tasks.
The BoW and LDA text models perform slightly better then the BoNG and PV models on most parameterizations.
The optimal encoding dimension of textual data varies depending on the data set.

The cycle time prediction on the hospital admission event log is noticeably worse, if textual data is considered.
This shows that text-aware process prediction does not necessarily guarantee predication improvements.
On the real-world event log the text-aware models could capitalize mostly on short trace prefixes.
A potential explanation is that more training data for short trace prefixes is available such that it is easier to generalize for shorter prefixes.