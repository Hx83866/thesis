This chapters discusses scientific contributions to process prediction and text mining that are related to this thesis.
%Both disciplines have a history have a ra

\section{Contributions to Process Prediction}

The prediction of the future course of a process instance has been an important subfield in process mining research, aiming to enhance process monitoring capabilities.
Depending on the use case, for example, predicting time-related attributes, the next activity or the outcome of a case can be of interest.
Most approaches presented in the literature either use process models or machine learning methods to construct a predictor, which generalizes from a historical event log.

Five different non-parametric regression predictors for forecasting the cycle time of an unfinished case have been presented by \Citeauthor{DBLP:conf/otm/DongenCA08} \cite{DBLP:conf/otm/DongenCA08}.
The estimates are based on activity occurrences, activity duration, and other attributes.

Furthermore, \Citeauthor{DBLP:journals/is/AalstSS11} proposed building a transition system using a set, bag, or sequence abstraction, which is annotated with time-related data to predict the cycle time of case \cite{DBLP:journals/is/AalstSS11}.
The core idea of this approach is to replay unfinished cases on the learned transition system and compute the prediction using the historical measurements in the annotations.

\citeauthor{DBLP:conf/colcom/PandeyNC11} use a hidden Markov model to predict the cycle time of a case using the activity and timestamp data of an event log \cite{DBLP:conf/colcom/PandeyNC11}.

\citeauthor{DBLP:conf/icsoc/Rogge-SoltiW13} showed how a stochastic Petri net could be used to predict the cycle time of a process instance \cite{DBLP:conf/icsoc/Rogge-SoltiW13}.
The model naturally supports parallelism in business processes and considers future events, which are expected to occur. 

\citeauthor{DBLP:conf/dis/CeciLFCM14} presented an approach where a sequence tree is learned to relate running traces to similar historical traces \cite{DBLP:conf/dis/CeciLFCM14}.
A decision tree is then used to predict the next activity and the cycle time of a case.

\begin{table}[htbp!]
	\renewcommand{\arraystretch}{1.5}
	\begin{tabularx}{\textwidth}{
			>{\hsize=2.0\hsize}X
			>{\hsize=0.4\hsize}X
			>{\hsize=1.3\hsize}X
			>{\hsize=0.5\hsize}X
			>{\hsize=0.5\hsize}X
			>{\hsize=1.3\hsize}X
		}
		\toprule
		\textbf{Contribution} & \textbf{Year} & \textbf{Model(s)}  & \textbf{Data-Aware} &  \textbf{Text-Aware} & \textbf{Predictions} \\ \midrule
		Van Dongen et al. \cite{DBLP:conf/otm/DongenCA08}& \citeyear{DBLP:conf/otm/DongenCA08} & Regression  & \checkmark & \xmark& Cycle time\\
		
		Van der Aalst et al. \cite{DBLP:journals/is/AalstSS11}&  \citeyear{DBLP:journals/is/AalstSS11}& Transition system  
		& \xmark & \xmark & Cycle time \\   
		
		\citeauthor{DBLP:conf/colcom/PandeyNC11} \cite{DBLP:conf/colcom/PandeyNC11} & \citeyear{DBLP:conf/colcom/PandeyNC11} & Hidden Markov & \xmark & \xmark & Cycle time \\
		
		\citeauthor{DBLP:conf/icsoc/Rogge-SoltiW13} \cite{DBLP:conf/icsoc/Rogge-SoltiW13} & \citeyear{DBLP:conf/icsoc/Rogge-SoltiW13} &Stochastic Petri net & \xmark & \xmark & Cycle time\\
		
		\citeauthor{DBLP:conf/dis/CeciLFCM14} \cite{DBLP:conf/dis/CeciLFCM14} & \citeyear{DBLP:conf/dis/CeciLFCM14} & Sequence tree \newline Decision tree& \checkmark & \xmark & Next activity \newline Cycle time \\
		
		\citeauthor{DBLP:conf/bpm/TeinemaaDMF16}  \cite{DBLP:conf/bpm/TeinemaaDMF16} &  \citeyear{DBLP:conf/bpm/TeinemaaDMF16} & Random forest \newline Logistic regression & \checkmark & \checkmark & Outcome \\
		
		\citeauthor{ DBLP:conf/bpm/EvermannRF16} \cite{ DBLP:conf/bpm/EvermannRF16} &  \citeyear{ DBLP:conf/bpm/EvermannRF16}& LSTM & \xmark & \xmark & Next activity \\
		
		\citeauthor{DBLP:conf/caise/TaxVRD17} \cite{DBLP:conf/caise/TaxVRD17} & \citeyear{DBLP:conf/caise/TaxVRD17} & LSTM & \xmark & \xmark & Next activity \newline Next event time \newline Cycle time \newline Future path \\
		\citeauthor{DBLP:conf/ssci/NavarinVPS17} \cite{DBLP:conf/ssci/NavarinVPS17} &  \citeyear{DBLP:conf/ssci/NavarinVPS17}&  LSTM & \checkmark  & \xmark & Cycle time\\
		
		\citeauthor{DBLP:journals/computing/PolatoSBL18} \cite{DBLP:journals/computing/PolatoSBL18}&  \citeyear{DBLP:journals/computing/PolatoSBL18} &  Transition system \newline SVR \newline Naive Bayes &  \checkmark & \xmark & Next activity \newline Cycle time \newline Future path \\
		
		\citeauthor{DBLP:conf/icpm/ParkS19} \cite{DBLP:conf/icpm/ParkS19} & \citeyear{DBLP:conf/icpm/ParkS19} & LSTM & \checkmark & \xmark &Next activity \newline Next event time  \\
		
		This contribution &  2020 &  LSTM & \checkmark & \checkmark &  Next activity \newline Next event time  \newline Cycle time \newline Outcome
		\\ \bottomrule
	\end{tabularx}
	\caption[Comparison  of process prediction methods]{Comparison  of process prediction methods.}
	\label{tab:preliminaries}
\end{table}

\citeauthor{DBLP:conf/bpm/TeinemaaDMF16} applied text vectorization techniques like Bag of N-Gram, Latent Dirichlet Allocation and Paragraph Vectors to textual data of processes in order to predict a binary label describing the process outcome \cite{DBLP:conf/bpm/TeinemaaDMF16}.
In this approach, event traces are encoded as vectors.
Then, random forest and logistic regression classifiers are trained for each prefix of a trace.

Most recently, several authors have applied recurrent neural networks in the form of LSTM networks for process prediction. \citeauthor{ DBLP:conf/bpm/EvermannRF16} encode events using an embedding matrix as it is known for word embeddings.
The embedded events are then used as input for an LSTM network that predicts the next activity \cite{DBLP:conf/bpm/EvermannRF16}.

\citeauthor{DBLP:conf/caise/TaxVRD17} use a one-hot encoding of the activity and the timestamp of an event to predict the activity and timestamp of the next event \cite{DBLP:conf/caise/TaxVRD17}.
This is done by using a two-layered LSTM network architecture.

The work by \citeauthor{DBLP:conf/ssci/NavarinVPS17} adopted the idea of using an LSTM network \cite{DBLP:conf/caise/TaxVRD17} and extends the encoding by also utilizing additional data attributes associated with each event \cite{DBLP:conf/ssci/NavarinVPS17} to predict the cycle time of a case.

\citeauthor{DBLP:journals/computing/PolatoSBL18} presented a set of approaches that use support vector regression for cycle time prediction  \cite{DBLP:journals/computing/PolatoSBL18}.
The authors implement different encodings for events in this contribution including a simple one-hot encoding and a more advanced state-based encoding using transition systems.
Furthermore, they enhance the approach in \cite{DBLP:journals/is/AalstSS11} by taking additional data attributes into account.

\citeauthor{DBLP:journals/tkdd/TeinemaaDRM19} reported an in-depth review and benchmark of outcome-oriented predictive process monitoring approaches.
The study showed that aggregated encodings like counting frequencies of activities are the most reliable encoding for predicting the outcome of a case \cite{DBLP:journals/tkdd/TeinemaaDRM19}.

\citeauthor{DBLP:conf/icpm/ParkS19} showed how LSTM-based predictions could be used to solve a resource allocation problem, leading to direct recommendations for process improvement \cite{DBLP:conf/icpm/ParkS19}.

The most extensive benchmark of sequential prediction models has been realized by \citeauthor{DBLP:journals/sosym/TaxTZ20} \cite{DBLP:journals/sosym/TaxTZ20}.
The authors show that black-box process prediction methods from the machine learning field outperform process model-oriented techniques on next-element prediction tasks.
However, the latter are more efficient and offer higher interpretability at the cost of prediction quality.

%The main difference between process model-based and machine learning-based techniques 

A comparison of the process prediction methods is presented in Table \ref{tab:preliminaries}.
The table shows the underlying model that is used to generalize from historical event data for each approach.
In addition, it states whether the methods use additional textual or non-textual data, and which prediction targets are supported.

\section{Contributions to Text Mining and Natural Language Processing}

The statistical analysis of texts as one of the earliest applications of text mining dates back to the pre-computer era.
In \citeyear{mendenhall1887characteristic}, \citeauthor{mendenhall1887characteristic} analyzed the distribution of word lengths to determine the authorship of literary works \cite{mendenhall1887characteristic}.
Through the comparison of the authorsâ€™ writing characteristics, controversial authorship can be validated or falsified to a certain extent.

In the 1960s, text mining became relevant alongside with \textit{information retrieval} (IR), a research field that deals with concepts of searchable data bases \cite{DBLP:journals/debu/Singhal01}.
The SMART system \cite{DBLP:journals/cacm/SaltonL65} by \citeauthor{DBLP:journals/cacm/SaltonL65} was an milestone in IR, which introduced an information retrieval system using a vector space model.
The vector space model is a widely adopted model that represents text documents and search queries as vectors based on term and document frequencies \cite{DBLP:journals/cacm/SaltonWY75}.

With the rise of machine learning methods, word and document embeddings became an important subfield in text mining and natural language processing.
Statistical topic models, like Latent Dirichlet Allocation (LDA) \cite{DBLP:journals/jmlr/BleiNJ03}, have been utilized to represent documents as a mixture over latent topics.
The topics are distributions overs words, which can be derived using statistical inference together with the document mixtures.

Furthermore, neural networks have been applied to compute word \cite{DBLP:journals/corr/abs-1301-3781} and document embeddings \cite{DBLP:conf/icml/LeM14} using supervised learning.
A wide range of network architectures have been proposed that learn distributed vector representations by predicting words using their contexts or vice versa.
These methods map words or documents to a lower dimensional but continuous vector space.
Then, these vectors can be utilized for other machine learning problems.

Most recently, pre-trained \textit{transformer} models, like BERT \cite{DBLP:conf/naacl/DevlinCLT19} by \citeauthor{DBLP:conf/naacl/DevlinCLT19}, achieve state-of-the-art performance on many natural language processing tasks.
These highly parallelizable deep learning models are pre-trained on large text corpora (unsupervised) and then fine-tuned to solve text-related supervised learning problems, like next-sentence prediction or reading comprehension.
The underlying concepts of the model that lead to the high performance are not yet completely understood \cite{DBLP:conf/emnlp/KovalevaRRR19}.