
Text-aware process prediction requires to combine methods from several scientific fields.
Therefore, in this chapter, scientific contributions to process prediction, text mining, natural language processing, and supervised learning that are related to this thesis are discussed.

\section{Contributions to Process Prediction}

The prediction of the future course of a process instance has been an important subfield in process mining research, aiming to enhance process monitoring capabilities.
Depending on the use case, for example, predicting time-related attributes, the next activity or the outcome of a case can be of interest.
Most approaches presented in the literature either use process models or machine learning methods to construct a predictor, which generalizes from a historical event log.

Five different non-parametric regression predictors for forecasting the cycle time of an unfinished case have been presented by \Citeauthor{DBLP:conf/otm/DongenCA08} \cite{DBLP:conf/otm/DongenCA08}.
The estimates are based on activity occurrences, activity duration, and other attributes.

Furthermore, \Citeauthor{DBLP:journals/is/AalstSS11} proposed building a transition system using a set, bag, or sequence abstraction, which is annotated with time-related data to predict the cycle time of case \cite{DBLP:journals/is/AalstSS11}.
The core idea of this approach is to replay unfinished cases on the learned transition system and compute the prediction using the historical measurements in the annotations.

\citeauthor{DBLP:conf/colcom/PandeyNC11} use a hidden Markov model to predict the cycle time of a case using the activity and timestamp data of an event log \cite{DBLP:conf/colcom/PandeyNC11}.

\citeauthor{DBLP:conf/icsoc/Rogge-SoltiW13} showed how a stochastic Petri net could be used to predict the cycle time of a process instance \cite{DBLP:conf/icsoc/Rogge-SoltiW13}.
The model naturally supports parallelism in business processes and considers future events, which are expected to occur. 

\citeauthor{DBLP:conf/dis/CeciLFCM14} presented an approach where a sequence tree is learned to relate running traces to similar historical traces \cite{DBLP:conf/dis/CeciLFCM14}.
A decision tree is then used to predict the next activity and the cycle time of a case.

\begin{table}[htbp!]
	\renewcommand{\arraystretch}{1.5}
	\begin{tabularx}{\textwidth}{
			>{\hsize=2.0\hsize}X
			>{\hsize=0.4\hsize}X
			>{\hsize=1.3\hsize}X
			>{\hsize=0.5\hsize}X
			>{\hsize=0.5\hsize}X
			>{\hsize=1.3\hsize}X
		}
		\toprule
		\textbf{Contribution} & \textbf{Year} & \textbf{Model(s)}  & \textbf{Data-Aware} &  \textbf{Text-Aware} & \textbf{Predictions} \\ \midrule
		Van Dongen et al. \cite{DBLP:conf/otm/DongenCA08}& \citeyear{DBLP:conf/otm/DongenCA08} & Regression  & \checkmark & \xmark& Cycle time\\
		
		Van der Aalst et al. \cite{DBLP:journals/is/AalstSS11}&  \citeyear{DBLP:journals/is/AalstSS11}& Transition system  
		& \xmark & \xmark & Cycle time \\   
		
		\citeauthor{DBLP:conf/colcom/PandeyNC11} \cite{DBLP:conf/colcom/PandeyNC11} & \citeyear{DBLP:conf/colcom/PandeyNC11} & Hidden Markov & \xmark & \xmark & Cycle time \\
		
		\citeauthor{DBLP:conf/icsoc/Rogge-SoltiW13} \cite{DBLP:conf/icsoc/Rogge-SoltiW13} & \citeyear{DBLP:conf/icsoc/Rogge-SoltiW13} &Stochastic Petri net & \xmark & \xmark & Cycle time\\
		
		\citeauthor{DBLP:conf/dis/CeciLFCM14} \cite{DBLP:conf/dis/CeciLFCM14} & \citeyear{DBLP:conf/dis/CeciLFCM14} & Sequence tree \newline Decision tree& \checkmark & \xmark & Next activity \newline Cycle time \\
		
		\citeauthor{DBLP:conf/bpm/TeinemaaDMF16}  \cite{DBLP:conf/bpm/TeinemaaDMF16} &  \citeyear{DBLP:conf/bpm/TeinemaaDMF16} & Random forest \newline Logistic regression & \checkmark & \checkmark & Outcome \\
		
		\citeauthor{ DBLP:conf/bpm/EvermannRF16} \cite{ DBLP:conf/bpm/EvermannRF16} &  \citeyear{ DBLP:conf/bpm/EvermannRF16}& LSTM & \xmark & \xmark & Next activity \\
		
		\citeauthor{DBLP:conf/caise/TaxVRD17} \cite{DBLP:conf/caise/TaxVRD17} & \citeyear{DBLP:conf/caise/TaxVRD17} & LSTM & \xmark & \xmark & Next activity \newline Next event time \newline Cycle time \newline Future path \\
		\citeauthor{DBLP:conf/ssci/NavarinVPS17} \cite{DBLP:conf/ssci/NavarinVPS17} &  \citeyear{DBLP:conf/ssci/NavarinVPS17}&  LSTM & \checkmark  & \xmark & Cycle time\\
		
		\citeauthor{DBLP:journals/computing/PolatoSBL18} \cite{DBLP:journals/computing/PolatoSBL18}&  \citeyear{DBLP:journals/computing/PolatoSBL18} &  Transition system \newline SVR \newline Naive Bayes &  \checkmark & \xmark & Next activity \newline Cycle time \newline Future path \\
		
		\citeauthor{DBLP:conf/icpm/ParkS19} \cite{DBLP:conf/icpm/ParkS19} & \citeyear{DBLP:conf/icpm/ParkS19} & LSTM & \checkmark & \xmark &Next activity \newline Next event time  \\
		
		This contribution &  2020 &  LSTM & \checkmark & \checkmark &  Next activity \newline Next event time  \newline Cycle time \newline Outcome
		\\ \bottomrule
	\end{tabularx}
	\caption[Comparison  of process prediction methods]{Comparison  of process prediction methods.}
	\label{tab:preliminaries}
\end{table}

\citeauthor{DBLP:conf/bpm/TeinemaaDMF16} applied text vectorization techniques like Bag of N-Gram, Latent Dirichlet Allocation and Paragraph Vectors to textual data of processes in order to predict a binary label describing the process outcome \cite{DBLP:conf/bpm/TeinemaaDMF16}.
In this approach, event traces are encoded as vectors.
Then, random forest and logistic regression classifiers are trained for each prefix of a trace.

Most recently, several authors have applied recurrent neural networks in the form of LSTM networks for process prediction. \citeauthor{ DBLP:conf/bpm/EvermannRF16} encode events using an embedding matrix as it is known for word embeddings.
The embedded events are then used as input for an LSTM network that predicts the next activity \cite{DBLP:conf/bpm/EvermannRF16}.

\citeauthor{DBLP:conf/caise/TaxVRD17} use a one-hot encoding of the activity and the timestamp of an event to predict the activity and timestamp of the next event \cite{DBLP:conf/caise/TaxVRD17}.
This is done by using a two-layered LSTM network architecture.

The work by \citeauthor{DBLP:conf/ssci/NavarinVPS17} adopted the idea of using an LSTM network \cite{DBLP:conf/caise/TaxVRD17} and extends the encoding by also utilizing additional data attributes associated with each event \cite{DBLP:conf/ssci/NavarinVPS17} to predict the cycle time of a case.

\citeauthor{DBLP:journals/computing/PolatoSBL18} presented a set of approaches that use support vector regression for cycle time prediction  \cite{DBLP:journals/computing/PolatoSBL18}.
The authors implement different encodings for events in this contribution including a simple one-hot encoding and a more advanced state-based encoding using transition systems.
Furthermore, they enhance the approach in \cite{DBLP:journals/is/AalstSS11} by taking additional data attributes into account.

\citeauthor{DBLP:journals/tkdd/TeinemaaDRM19} reported an in-depth review and benchmark of outcome-oriented predictive process monitoring approaches.
The study showed that aggregated encodings like counting frequencies of activities are the most reliable encoding for predicting the outcome of a case \cite{DBLP:journals/tkdd/TeinemaaDRM19}.

\citeauthor{DBLP:conf/icpm/ParkS19} showed how LSTM-based predictions could be used to solve a resource allocation problem, leading to direct recommendations for process improvement \cite{DBLP:conf/icpm/ParkS19}.

The most extensive benchmark of sequential prediction models has been realized by \citeauthor{DBLP:journals/sosym/TaxTZ20} \cite{DBLP:journals/sosym/TaxTZ20}.
The authors show that black-box process prediction methods from the machine learning field outperform process model-oriented techniques on next-element prediction tasks.
However, the latter are more efficient and offer higher interpretability at the cost of prediction quality.

%The main difference between process model-based and machine learning-based techniques 

A comparison of the process prediction methods is presented in Table \ref{tab:preliminaries}.
The table shows the underlying model that is used to generalize from historical event data for each approach.
In addition, it states whether the methods use additional textual or non-textual data, and which prediction targets are supported.

\section{Contributions to Text Mining and Natural Language Processing}

The statistical analysis of texts is one of the earliest applications of text mining and dates back to the pre-computer era.
In \citeyear{mendenhall1887characteristic}, \citeauthor{mendenhall1887characteristic} analyzed the distribution of word lengths to determine the authorship of literary works \cite{mendenhall1887characteristic}.
Through the comparison of the authorsâ€™ writing characteristics, controversial authorship can be validated or falsified to a certain extent.

In the 1960s, text mining became relevant alongside with \textit{information retrieval} (IR), a research field that deals with concepts of searchable data bases \cite{DBLP:journals/debu/Singhal01}.
The SMART system \cite{DBLP:journals/cacm/SaltonL65} by \citeauthor{DBLP:journals/cacm/SaltonL65} was an milestone in IR, which introduced an information retrieval system using a vector space model.
The vector space model is a widely adopted model that represents text documents and search queries as vectors based on term and document frequencies \cite{DBLP:journals/cacm/SaltonWY75}.

With the rise of machine learning methods, word and document embeddings became an important subfield in text mining and natural language processing.
Statistical topic models, like Latent Dirichlet Allocation (LDA) \cite{DBLP:journals/jmlr/BleiNJ03}, have been utilized to represent documents as a mixture over latent topics.
The topics are distributions overs words, which can be derived using statistical inference together with the document mixtures.

Furthermore, neural networks have been applied to compute word \cite{DBLP:journals/corr/abs-1301-3781} and document embeddings \cite{DBLP:conf/icml/LeM14} using supervised learning.
A wide range of network architectures have been proposed that learn distributed vector representations by predicting words using their contexts or vice versa.
These methods map words or documents to a lower dimensional but continuous vector space.
Then, these vectors can be utilized for other machine learning problems.

Most recently, pre-trained \textit{transformer} models, like BERT \cite{DBLP:conf/naacl/DevlinCLT19} by \citeauthor{DBLP:conf/naacl/DevlinCLT19}, achieve state-of-the-art performance on many natural language processing tasks.
These highly parallelizable deep learning models are pre-trained on large text corpora (unsupervised) and then fine-tuned to solve text-related supervised learning problems, like next-sentence prediction or reading comprehension.
The underlying concepts of the model that lead to the high performance are not yet completely understood \cite{DBLP:conf/emnlp/KovalevaRRR19}.

\section{Contributions to Supervised Learning}

The foundations of supervised learning were laid with \textit{Bayes' theorem} published in \citeyear{bayes1763lii} \cite{bayes1763lii}.
This theorem was the basis for a wide range of statistical Bayesian methods, like the naive Bayesian classifiers, which can be used to address supervised learning problems.
The naive Bayesian methods assume independence of the feature variables to efficiently classify instances based on conditional probabilities \cite{hand2001idiot}.

Another early application of supervised learning is the \textit{least square regression} method.
It can be used to fit the parameters of a function to a data set of points by minimizing the sum of squared residuals.
The method was originally published by \citeauthor{legendre1805nouvelles} in \citeyear{legendre1805nouvelles} \cite{legendre1805nouvelles}.
However, there are several indications that GauÃŸ used least square regression ten years before \citeauthor{legendre1805nouvelles} \cite{stigler1981gauss}.

In \citeyear{rosenblatt1957perceptron}, the \textit{perceptron} algorithm was invented by \citeauthor{rosenblatt1957perceptron} \cite{rosenblatt1957perceptron}.
The perceptron is binary classifier that can learn linearly separable patterns.
As a single-layer feedforward neural network, the perceptron is not able to learn functions that are not linearly separable, like the XOR function \cite{minsky2017perceptrons}.
However, with the introduction of multi-layer neural networks, this limitation could be overcome.
Using the backprogatation algorithm, multi-layer neural networks can approximate any continuous functions within a compact set \cite{DBLP:journals/mcss/Cybenko89}.

Alongside neural networks, decision trees \cite{DBLP:journals/ml/Quinlan86} and support vector machines \cite{DBLP:journals/ml/CortesV95} became popular.
Decision trees are used to classify instances in the leafs of a tree data structure in dependence of their feature variables.
The tree can be computed, inter alia, by iteratively splitting tree nodes based on the feature variables of their instances, so that the information gain is maximized.
Support vector machines are classifiers that separate instances by a hyperplane, so that the margin between the hyperplane and the instances is maximized.

With recurrent neural networks (RNNs), a new class of neural networks was introduced, which is able to process sequences of input data \cite{rumelhart1986learning}.
By that, these networks can learn dynamic behavior.
Unlike feedforward neural networks, reccurrent neural networks keep a state across inputs.
The LSTM neural network is one of the most popular RNN architectures available, which addresses the vanishing gradient problem of many-layered and recurrent neural networks \cite{DBLP:journals/neco/HochreiterS97}.

Today, a rapidly expanding set of supervised learning methods is available to solve complex tasks in computer vision, natural language processing, and many other fields.