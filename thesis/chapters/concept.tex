Text-aware process prediction aims to utilize unstructured text information in event data to improve case predictions.
While many prediction methods have been applied to event data, almost none of them are able to handle textual data.
An exception is the approach presented in \cite{DBLP:conf/bpm/TeinemaaDMF16}, where traces are encoded as fixed vectors and a classifier is learned for each prefix length.

In this chapter a novel approach for process prediction is presented that is able to handle numerical, categorical and textual data, captures temporal dependencies and concept drifts using an event-wise encoding and a sequential prediction model (LSTM).

\section{Overview}





\section{Log Encoding}


Given an event log $\eventlog = \{\sigma_1, \dots, \sigma_l\}$ with historical event data, the set of all prefix traces $\{\sigma(1,k) \mid \forall \sigma \in \eventlog, 1 \leq k \leq |\sigma|\}$ is computed.
Each prefix corresponds to one training example for the prediction model.
The total number of training examples that can be generated out of the log is $\sum_{\sigma \in \eventlog}|\sigma|$, which is exactly the number of events in the log.

\section{Event Encoding}

Each event is encoded in a fixed-length vector using the event's activity, timestamp and additional categorical, numerical and textual attributes.
The encoding of each attributes 

$ encode(e_i)$
\subsection{Encoding of Activities and other Categorical Attributes}



\subsection{Encoding of Time and other numerical Attributes}

For timestamp prediction of the next and final event of running process instance a set of time-based features is computed from the timestamp data in the event log.
Given an event log $\eventlog$ an event $e_i$ from a trace $\sigma = \langle e_1, \dots, e_n \rangle$ the following time features are computed for the encoding of $e_i$: 

\begin{center}
\begin{tabularx}{\textwidth}{l l}
	\centering
	 \textbf{Feature} & \textbf{Description} \\
	$t_1 = \pi_\mathcal{T}(e_i) - \pi_\mathcal{T}(e_{i-1})$ & Seconds since previous event \\
	$t_2 = \pi_\mathcal{T}(e_i) - \pi_\mathcal{T}(e_1)$ & Seconds since case start \\
	$t_3 = \pi_\mathcal{T}(e_i) - \min\{\pi_\mathcal{T}(e_{j}), \forall e_j \in \sigma_k, \forall  \sigma_k \in \eventlog\}$ & Seconds since first recorded event \\
	$t_4$ & Seconds since midnight \\
	$t_5$ & Seconds since last Monday \\
	$t_6$ & Seconds since last January 1 00:00
\end{tabularx}
\end{center}

Using the time features a set of time-dependent trends can be captured and utilized for prediction.
The features $t_1$ and $t_2$ give information about the time between events and the time of the event in the case.
Using $t_3$ the absolute time position of an event in the data can be determined.
This is important to detect concept drift in the process.
The features $t_4, t_5$ and $t_6$ are used to capture daily, weekly or seasonal trends.
For example, some activities might only be executed during office hours, before the weekend, during summer.

Each feature $t_1, \dots t_6$ as well as all additional numerical attributes $d_i$ are scaled to the interval $ [0, 1]$ to improve learning efficiency using min-max normalizing.
The scaling for a numerical feature $x$ is realized with the transformation

$$\hat{x} = \dfrac{x-\min(x)}{\max(x) - \min(x)} \in [0, 1],$$

where $\min(x)$ is the lowest and $\max(x)$ is the highest value $x$ can take.
If the limits are not bounded conceptually, the lowest or highest value of $x$ in the event log is used for scaling.

\subsection{Text Encoding}

In order to prepare the textual data of the event log for a prediction model, the texts have to be encoded in a "useful" numerical vector representation.
Useful in that context means, that texts with similar semantic meanings should also have similar representations.
Extracting the meaning of textual information remains a challenge even for humans, because textual data is unstructured, language dependent and domain specific.
In addition, grammatical variations and the importance of context in language makes text mining even more difficult.

The text vectorization is realized in a 2-step procedure.
First, all text data associated with the events in the corresponding textual attribute is collected in a so called \textit{text corpus}.
Each document in the text corpus is then preprocessed in order to filter out linguistic noise or useless information.
Finally, the text corpus is used to build up a vocabulary and a text vectorization technique is applied to encode the text attribute to a fixed of vector.

\subsubsection{Text Preprocessing}

In the preprocessing step each document is transformed by a processing pipeline which consists of the following four steps:

\begin{enumerate} 
	\item Letters are converted to lowercase
	\item Document is tokenized by word
	\item Each word is lemmatized
	\item Stop words are filtered out
\end{enumerate}

In the tokenenization step a document is split up in a list of words.
Each word is then lemmatized, i.e. it is converted to its canonical form.
The idea is to unify words that have a very similar meaning and filter out grammatical variations.
For example, the words  "go", "going", "goes", "gone" and "went" are all transformed to the basic form "go".
Ultimately, all stop words are filtered out of each document.
Stop words are words with low information value like "the", "a", "of" or "here".

Stop word lists are language dependent and can be more or less aggressive at filtering.
Usually they contains articles, auxiliary verbs, prepositions and generic verbs like "be" and "have".
In addition, punctuation marks or numerical information are excluded.

\begin{table}[]
	\begin{tabularx}{\textwidth}{l l p{9.8cm}}
		\toprule
		\textbf{Step} & \textbf{Transformation} & \textbf{Document}                                                       \\ \midrule
		0             & Original       & "The patient has been diagnosed with high blood pressure." \\
		1             & Lowercase               & "the patient has been diagnosed with high blood pressure." \\
		2 & Tokenization  & {[}"the", "patient", "has", "been", "diagnosed", "with", "high", "blood", "pressure", "."{]} \\
		3 & Lemmatization & {[}"the", "patient", "have", "be", "diagnose", "with", "high", "blood", "pressure", "."{]}   \\
		4             & Stop word filtering     & {[}"patient", "diagnose", "high", "blood", "pressure"{]} \\ \bottomrule
	\end{tabularx}
	\caption{Preprocessing transformation of an example document}
	\label{tab:text-preprocessing}
\end{table}








\subsubsection{Bag-of-N-Gram}
\subsubsection{Paragraph Vector}

\section{Network}