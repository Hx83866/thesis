Text-aware process prediction aims to utilize unstructured text information in event data to improve predictions for unfinished cases.
While many prediction methods have been applied to event data, almost none of them are able to handle textual data.
Nevertheless, a lot of textual information in the context of processes is available, for example in form of business emails or notes by employees or customers.
These texts in natural language might hold process-critical information and should consequently be considered for process prediction.
However, taking advantage of textual data remains a major challenge, since natural language is predominantly context-sensitive and ambiguous  \cite{textminingissues}.
A first approach has been presented by \citeauthor{DBLP:conf/bpm/TeinemaaDMF16}  \cite{DBLP:conf/bpm/TeinemaaDMF16}, where traces with textual data are encoded as vectors and a random forest classifier is learned for each prefix length.

In this chapter a novel approach for text-aware process prediction is presented that considers the control flow and additional numerical, categorical and textual data of the process.
The model is able to captures temporal dependencies between events, seasonal variability and concept drifts using an event-wise encoding and a sequential LSTM prediction model.
The main application scenario for the model is inside of real-time business process monitoring software, where prediction capabilities for running processes give a competitive advantage.

\section{Overview}

The framework is designed to approximate prediction functions $f_\mathrm{a}, f_\mathrm{t}, f_\mathrm{o}$ and $f_\mathrm{c}$, that predict the next \textbf{a}ctivity, next \textbf{t}imestamp, case \textbf{o}utcome and case \textbf{c}ycle time given any prefix $hd^k(\sigma)$ of length $1 \leq k \leq |\sigma|$ of the full (unkown) trace $\sigma = \langle e_1, \dots, e_{|\sigma|} \rangle \in \mathcal{E}^+$.
The next activity prediction function $f_\mathrm{a}\colon \mathcal{E}^+ \to \mathcal{A} \cup \{\text{END OF PROCESS}\}$ returns the activity of the next event or an artificial END OF PROCESS activity, if the given trace is already completed, precisely:
\begin{align*}
f_\mathrm{a}(hd^k(\sigma)) &= 
\begin{cases}
	\text{END OF PROCESS}& \text{if $k = |\sigma|$} \\
	\pi_\mathcal{A}(\sigma(k+1)) & \text{else} 
\end{cases}\\
\end{align*}
Furthermore, $f_\mathrm{t} \colon \mathcal{E}^+ \to \mathbb{R}$ gives the timestamp of the next event in relation to the last event in the prefix:
\begin{align*}
f_\mathrm{t}(hd^k(\sigma)) &=
\begin{cases}
\pi_\mathcal{T}(\sigma(|\sigma|)) - \pi_\mathcal{T}(\sigma(|\sigma| - 1))& \text{if $k = |\sigma|$} \\
\pi_\mathcal{T}(\sigma(k+1)) - \pi_\mathcal{T}(\sigma(k)) & \text{else} 
\end{cases}\\
\end{align*}
The case outcome of trace can be a binary label, ie. a label describing if the case has been successful or has failed.
In some applications the case outcome is defined by the activity of the final event of a case.
In that scenario,the outcome function $f_\mathrm{o} \colon \mathcal{E}^+ \to \mathcal{A}$ returns the last activity of the trace:
\begin{align*}
f_\mathrm{o}(hd^k(\sigma)) &= \pi_\mathcal{A}(\sigma(|\sigma|))\\
\end{align*}
Finally, $f_\mathrm{c} \colon \mathcal{E}^+ \to \mathbb{R}$ returns the total duration of the process, i.e. the time difference between the first and the last event of the trace:
\begin{align*}
f_\mathrm{c}(hd^k(\sigma)) &=  \pi_\mathcal{T}(\sigma(|\sigma|))-  \pi_\mathcal{T}(\sigma(1)).
\end{align*}

All functions are approximated via an LSTM model using historical event log data, i.e. a set of completed traces with full information about the course of the process instances.
The goal is to generalize from the completed traces in such a way, that the prediction error is minimized for new unseen and uncompleted traces.
Due to the probabilistic behavior of real-world processes, the prediction accuracy is always limited by the randomness of the process behavior.

The proposed framework (see Figure \ref{fig:framework}) consists of a preprocessing, encoding and prediction model component, which operate in an offline and online phase.
In the offline phase, a historical event log with completed traces of a business process is used to fit the encoding and prediction component.
Given a historical event log $\eventlog = \{\sigma_1, \dots, \sigma_l\}$ with completed traces, the set of all prefix traces $\eventlog_\mathrm{prefix} = \{ hd^k(\sigma) \mid  \sigma \in \eventlog, 1 \leq k \leq |\sigma|\}$ is computed and each trace is encoded as a sequence of event vectors.
The encoding component distinguish between categorical or numerical data that can be encoded directly and textual data, that is prepossessed and encoded based on a text model.
The text model is an exchangeable component and is fit to the text corpus, that is extracted from the textual data in the event log $\eventlog$.

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/framework}
	\caption[Overview of the text-aware process prediction framework]{Overview of the text-aware process prediction framework. Predictions for real-time processes are realized by an LSTM model that is fitted using an encoded representation of all prefixes of the historical event log. The encoding of textual attributes is realized by a text preprocessing pipeline and an exchangeable text encoding model.}
	\label{fig:framework}
\end{figure}

Each encoded prefix sequence with its desired prediction target values according to $f_\mathrm{a}, f_\mathrm{t}, f_\mathrm{o}$ and $f_\mathrm{c}$ corresponds to one training example for an LSTM network, that finally realizes the predictions. 
The total number of training examples that can be generated out of the log is $\sum_{\sigma \in \eventlog}|\sigma|$, which is exactly the number of events in the log.

In the online phase, the model applies the learned prediction function to predict the activity and time of the next event as well as the outcome and cycle time of new unseen and unfinished traces (real-time event log), that are monitored by the business process monitoring system.

\section{Event Encoding}\label{sec:event}

In the offline training phase as well as during online prediction, traces are encoded as sequences of event vectors.
The prefix log $\eventlog_\mathrm{prefix}$ is encoded as training set in the offline phase, while in the online phase, running cases are encoded for prediction.
Strictly speaking, an encoding function is realized by the encoding component, that transforms (prefix) traces of length $k$ to vector sequences of the same size, i.e. $enc(\sigma) = \langle \vec{x_1}, \vec{x_2}, \dots, \vec{x_k}\rangle$ with $\sigma = \langle e_1, e_2, \dots, e_k\rangle$ for $k \in \mathbb{N}$.
Each event $e_i$ is encoded as a fixed-length vector using the activity, timestamp and additional categorical, numerical and textual data, that is associated with each event.
We assume to have $r \in \mathbb{N}_0$ numerical, $s \in \mathbb{N}_0$ categorical and $u \in \mathbb{N}_0$ textual attributes, i.e. $e_i \in \mathcal{C} \times \mathcal{A}  \times \mathcal{T} \times \mathcal{D}_1^\mathrm{num} \times \dots \times \mathcal{D}_r^\mathrm{num}   \times \mathcal{D}_1^\mathrm{cat}  \times  \dots  \times \mathcal{D}_s^\mathrm{cat}   \times \mathcal{D}_1^\mathrm{text}   \times \dots  \times \mathcal{D}_u^\mathrm{text}$.
Each encoded event vector $\vec{x_i}$ is the concatenation of set of feature vectors, which are constructed from the event data.
\begin{equation*}
\vec{x_i}=(
\vec{a_i},
\vec{t_i},
\vec{d}_{i1}^\mathrm{num}, \dots,\vec{d}_{ir}^\mathrm{num},
\vec{d}_{i1}^\mathrm{cat}, \dots,\vec{d}_{is}^\mathrm{cat},
\vec{d}_{i1}^\mathrm{text}, \dots, \vec{d}_{iu}^\mathrm{text})
\end{equation*}
The activity of the event is represented by vector $\vec{a_i}$ using \textit{one-hot encoding}.
More precisely, given the set of possible activities $\mathcal{A}$, an arbitrary but fixed ordering over $\mathcal{A}$ is introduced with a bijective index function $index_\mathcal{A} \colon \mathcal{A} \to \{1, \dots, |\mathcal{A}|\}$.
Using this function, the activity is encoded as a vector of size $|\mathcal{A}|$, where the component $index_\mathcal{A}(\pi_\mathcal{A}(e))$ has value 1 and all the other components have value 0.
We write $\mathds{1}_\mathcal{A}\colon \mathcal{A} \to \{0,1\}^\mathcal{A}$ to describe the function that realizes such an one-hot encoding transformation for the set of all activities $\mathcal{A}$.
The timestamp of the event is used to compute a six-dimensional vector $\vec{t_i}$ of time-related features, which is explained in detail in Section \ref*{sec:time}.

Additional attributes of the events are encoded based on their type, i.e. if they are numerical, categorical or textual.
Categorical attributes are encoded using one-hot encoding in the same way as the activity, i.e. $\vec{d}_{ij}^\mathrm{cat} = \mathds{1}_{\mathcal{D}_j^{\mathrm{cat}}}(\pi_{\mathcal{D}_j^\mathrm{cat}}(e_i))$.
Textual attributes are vectorized via a dedicated text model, that is explained in Section \ref{sec:text}.
The dimensions of the text vectorizations can be tuned individually per attribute using the parameter $z_j$, which is the encoding dimension of the $j$-th textual attribute.

\begin{table}[!htbp]
	\renewcommand{\arraystretch}{1.2}
	\setlength\tabcolsep{5pt}
	\begin{tabularx}{\textwidth}{
		>{\hsize=0.6\hsize}X
		>{\hsize=0.8\hsize}X
		>{\hsize=0.6\hsize}X
		>{\hsize=2.0\hsize}X
		}
		\toprule
		\textbf{Feature} \newline \textbf{Vector} & \textbf{Construction} &\textbf{Dimension} &  \textbf{Description} \\
		\midrule
		$\vec{a_i}$ &$\mathds{1}_\mathcal{A}(\pi_\mathcal{A}(e_i))$& $|\mathcal{A}|$& One-hot encoding of the activity. \\
		$\vec{t_i}$ & See Section \ref{sec:time} &6 & Time-based feature vector.\\
		$\vec{d}_{ij}^\mathrm{num}$ &  $\widehat{\pi_{\mathcal{D}_j^\mathrm{num}}(e_i)} $ &1 & Normalized value of the $j$-th numerical attribute\\
		$\vec{d}_{ij}^\mathrm{cat}$ & $\mathds{1}_{\mathcal{D}_j^{\mathrm{cat}}}(\pi_{\mathcal{D}_j^\mathrm{cat}}(e_i))$&$|\mathcal{D}_j^\mathrm{cat}|$ & One-hot encoding of the $j$-th categorical attribute.\\
		$\vec{d}_{ij}^\mathrm{text}$ & See Section \ref{sec:text} & $z_j$ \newline (parameter) & Fixed-length vectorization of the $j$-th text attribute.\\
		\bottomrule
	\end{tabularx}
	\caption[Feature vectors as part of the event encoding]{Feature vectors as part of the event encoding $\vec{x_i}$ for an event $e_i$.}
	\label{tab:features}
\end{table}

All additional numerical attributes $\pi_{\mathcal{D}_i^\mathrm{num}} (e_i)$ are scaled to the interval $ [0, 1]$ to improve learning efficiency using min-max normalizing.
The scaling for a numerical feature $x$ is realized with the transformation
\begin{equation*}
\hat{x} = \dfrac{x-\min(x)}{\max(x) - \min(x)},
\end{equation*}
where $\min(x)$ is the lowest and $\max(x)$ is the highest value $x$ can take.
If the limits are not bounded conceptually, the lowest or highest value of $x$ in the event log is used for scaling.

All in all, the encoding of an event $e_i$ results in a vector of size
\begin{equation*}
|\vec{x_i}|= |\mathcal{A}| + r + \sum_{i=1}^{s} |\mathcal{D}_i| + \sum_{j=1}^{u} z_j + 6.
\end{equation*}
An overview of each feature vector that is part of the event encoding is provided in Table \ref{tab:features}.

\section{Capturing Temporal Dependencies}\label{sec:time}

A set of time-based features is computed from the timestamp data in the event log in order to profit from time-related pattern in the process.
As part of the complete encoding $\vec{x}_i$ for an event $e_i$ in a (prefix) trace $\sigma = \langle e_1, \dots, e_k \rangle$, a time vector $\vec{t_i} = (t_i^1, t_i^2,t_i^3,t_i^4,t_i^5,t_i^6)$ of dimension 6 is computed.

\begin{table}[!htbp]
	\centering
	\begin{tabularx}{0.8\textwidth}{l l}
		\toprule
		 \textbf{Feature} & \textbf{Description} \\
		 \midrule
		$t_i^1$ & Normalized time in seconds since previous event \\
		$t_i^2$ & Normalized time in seconds since case start \\
		$t_i^3$ & Normalized time in seconds since first recorded event \\
		$t_i^4$ & Normalized time in seconds since midnight \\
		$t_i^5$ & Normalized time in seconds since last Monday \\
		$t_i^6$ & Normalized time in seconds since last January 1 00:00 \\
		\bottomrule
	\end{tabularx}
	\caption[Time-based features as part of the event encoding]{Time-based features as part of event encoding $\vec{x_i}$.}
	\label{tab:time-features}
\end{table}

Using these features time-related correlations can be captured and utilized for prediction.
The feature $t_i^1$ describes the time difference between the current event $e_i$ and previous event $e_{i-1}$, while $t_i^2$ gives the time difference between the current event and the first event of the case $e_1$, i.e. the time since the start of the case.
Furthermore, $t_i^3$ is the time difference between the current event and the first event that is recorded in the log. This feature indicates the absolute time position of an event in the data.
This information is important to detect concept drift \cite{DBLP:journals/tnn/BoseAZP14} in the process.
Most real-world processes are not static, i.e. the behavior of the process changes over time.
For example, in earlier process executions customers might have been informed by a letter, whereas in more recent cases customers are messaged using email or app notifications.
Therefore, the knowledge about the absolute time of the events can be used to relate cases in similar periods of time.

The features $t_i^4, t_i^5$ and $t_i^6$ describe the time of the event in relation to the day, week and year.
They are used to capture daily, weekly and seasonal trends.
For example, some activities might only be executed during office hours or before the weekend.
Also, many businesses expect seasonally fluctuating demand, for example a booking platform for vacation usually has much more customers in summer, which can affect the process execution in many ways.
Each feature $t_i^1, \dots, t_i^6$ is min-max normalized such that $t_i^j \in [0,1]$ for $j \in [6]$.
A summary of all time-related features can be seen in Table \ref{tab:time-features}.

\section{Text Vectorization}\label{sec:text}

In order to prepare the textual data of the event log for a prediction model, the texts have to be encoded in a compact, finite and useful numerical vector representation using a text model.
Useful in that context means, that texts with similar semantic meanings should also have similar representations.
The vector representation of textual data is an important aspect in \textit{Natural Language Processing} (NLP).
Extracting the meaning of textual information remains a challenge even for humans, because textual data is unstructured, language dependent and domain specific.
Many words are ambiguous, for example the word 'apple' might denote a fruit or a global technology company.
In addition, grammatical variations and the importance of context in language makes extracting the semantic meaning even more difficult for computers.

In our setting, the text vectorization for textual attributes is realized in a two step procedure.
First, all textual data associated with the events in the corresponding textual attribute is collected in a so called \textit{text corpus}.
Each document in the text corpus is then preprocessed in order to filter out linguistic noise or useless information.
This step is called \textit{text normalization}  \cite{DBLP:books/lib/JurafskyM09}.
Finally, the normalized text corpus is used to build up a \textit{vocabulary} and a text vectorization technique is applied to encode the text of the attribute into a fixed-length vector.
The vocabulary of a text corpus is a set $V$ of all relevant words (after filtering) that appear in the corpus, which is indexed by a bijective index function $index_V \colon V \to \{1, 2, \dots, |V|\}$.
As text vectorization techniques the \textit{Bag of Words}, \textit{Bag of N-Grams}, \textit{Paragraph Vector} and \textit{Latent Dirichlet Allocation} text models are considered, which are introduced in more detail in the Sections \ref{sec:bow} to \ref{sec:lda}.

\subsection{Text Normalization}

In the text normalization step each document of the text corpus is transformed by a preprocessing pipeline which consists of the following four steps:
\begin{enumerate} 
	\item All letters are converted to lowercase
	\item The document is tokenized (i.e. splitted) by word
	\item Each word is lemmatized
	\item Stop words are filtered out
\end{enumerate}
The first step eliminates all capital letters in the text.
In the tokenenization step a document is split up into a sequence of words.
Each word is then lemmatized, i.e. it is converted to its canonical form.
The idea is to unify words that have a very similar meaning and filter out grammatical variations.
For example, the words  'go', 'going', 'goes', 'gone' and 'went' are all transformed to the basic form 'go'.
Lemmatization significantly reduces the complexity of textual data accepting a hopefully small loss of information.

\begin{table}[!htbp]
	\begin{tabularx}{\textwidth}{l l p{9.8cm}}
		\toprule
		\textbf{Step} & \textbf{Transformation} & \textbf{Example Document}                                                       \\ \midrule
		0             & Original       & 'The patient has been diagnosed with high blood pressure.' \\
		1             & Lowercase               & 'the patient has been diagnosed with high blood pressure.' \\
		2 & Tokenization  & $\langle$'the', 'patient', 'has', 'been', 'diagnosed', 'with', 'high', 'blood', 'pressure', '.'$\rangle$ \\
		3 & Lemmatization & $\langle$'the', 'patient', 'have', 'be', 'diagnose', 'with', 'high', 'blood', 'pressure', '.'$\rangle$  \\
		4             & Stop word filtering     & $\langle$'patient', 'diagnose', 'high', 'blood', 'pressure'$\rangle$ \\ \bottomrule
	\end{tabularx}
	\caption[Preprocessing transformation of an example document]{Preprocessing transformation of an example document containing a single sentence.}
	\label{tab:text-preprocessing}
\end{table}

Ultimately, all stop words are filtered out of each document.
Stop words are words with low information value like 'the', 'a', 'of' or 'here'.
Stop word lists are language dependent and can be more or less aggressive at filtering.
Usually they contains articles, auxiliary verbs, prepositions and generic verbs like 'be' and 'have'.
In addition, punctuation marks and numerical symbols are excluded.
An example application of the text normalization pipeline is shown in Table \ref{tab:text-preprocessing}.
Using the corpus of normalized documents (i.e. texts), a text model is fitted in the offline phase to realized the encoding of the textual attributes.

\subsection{Bag of Words}\label{sec:bow}

The \textit{Bag of Words} (BoW) model is a simple text model, which represents documents based on the \textit{term frequencies} of its words, while ignoring their order \cite{harris1954distributional}.
Given the learned vocabulary $V$, a document is represented by a vector of size $|V|$, where the $i$-th component gives the number of occurrences of the word indexed with $i$ in the document.

Since this approach does not reflect the prior distributions of words in the corpus, i.e. how likely certain words appear in a document in general, the term frequencies are usually normalized by the so-called \textit{inverse document frequency} (idf) of a word.
The inverse document frequency indicates the specificity of a word in the corpus and is computed by dividing the total number of documents by the number of documents that contain the specific word and scaling that value logarithmically.
The resulting value is the tf-idf score of a word in a document.

The Bag of Words model is easy to build and effective for certain applications, but limited in several ways.
First, the model completely ignores the order of words, which is often crucial for understanding the semantic meaning. For example, the sentences 'The patient's health state went from stable to critical.' and 'The patient's health state went from critical to stable.' would result in the same vector representation, while the meaning is clearly inverted.
Second, the vector representations are sparse and of high dimensionality since they depend of the size of the vocabulary.
However, the dimension can be reduced by limiting the size of the vocabulary. For example, words with low term frequencies can be excluded from the vocabulary.
Unseen words, that appear during online prediction and are not part of the learned vocabulary are not considered.

\subsection{Bag of N-Grams}

The \textit{Bag of N-Grams} (BoNG) model is a generalization of the Bag of Words model, which addresses the missing word order awareness of the latter.
Instead of single words, the vocabulary consists of $n$-tuples of words, that appear consecutive in the corpus.
The unigram model ($n=1$) is equivalent to the BoW model.
For the bigram model ($n=2$), the vocabulary consists of pairs of words that appear next to each other in the documents.
For example, for the preprocessed document $\langle$'patient', 'diagnose', 'high', 'blood', 'pressure'$\rangle$, the pairs ('patient', 'diagnose'), ('diagnose', 'high'), ('high', 'blood') and ('blood', 'pressure') are taken into the vocabulary.
For $n>2$ $n$-tuples are generated accordingly.
The feature vector is constructed by computing the tf-idf score for each vocabulary entry like in the BoW model.

Compared to the BoW model, n-grams also take the order of words into account, which is beneficent in many scenarios.
However, the vocabulary size is usually even higher than in the BoW model.
In order to generate more compact vectors, distributed text representations are needed for larger text corpora and vocabularies.

\subsection{Paragraph Vector}

The \textit{Paragraph Vector} model (also known as \textit{Doc2Vec}), originally presented by \citeauthor{DBLP:conf/icml/LeM14} in \citeyear{DBLP:conf/icml/LeM14}, is an unsupervised algorithm that learns distributed fixed length vector representations for documents of variable length using a simple feedforward neural network \cite{DBLP:conf/icml/LeM14}.
The idea is inspired by the word embedding model presented by \citeauthor{DBLP:journals/jmlr/BengioDVJ03} \cite{DBLP:journals/jmlr/BengioDVJ03}, which can learn distributed fixed-length vector representations for words.
In this model words are mapped to vectors, that are trained to predict words from its context, i.e. words that appear before or after the target word in the training documents.
Several variants of this approach exits, notably the \textit{Continuous Bag of Words} model, which ignores the order of the words in the context and the \textit{Continuous Skip-gram} model, which predicts the skip-gram context for a word vector (also known as \textit{Word2Vec}) \cite{DBLP:journals/corr/abs-1301-3781}.

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=\textwidth]{figures/paragraph-vector}
	\caption[Distributed memory Paragraph Vector model]{The distributed memory Paragraph Vector model (PV-DM) is designed to predict a word $w_t$ from its context and derives fixed-length representation of documents and words via the learned matrices $\vec{D}$ and $\vec{W}$. The dimension of the representations are defined before by parameter $h$.}
	\label{fig:paragraph-vector}
\end{figure}

The core idea of the Paragraph Vector model is to extend the model in \cite{DBLP:journals/jmlr/BengioDVJ03} in a way, that an additional document (i.e. paragraph) vector is trained together with the word vectors, that is unique per document.
Fig. \ref{fig:paragraph-vector} show the architecture of the distributed memory variant of the Paragraph Vector model (PV-DM).
Its is realized by a neural network, that takes one-hot encoded words and an one-hot encoded document ID as input.
These are mapped to vector representation via weight matrices $\vec{D}$ and $\vec{W}$, which are learned during training with gradient descent.
The distributed representations are then averaged or concatenated to a vector in order to predict the one-hot encoded target word using another mapping via $\vec{W'}$ and a softmax activation function.
The training set is constructed using a sliding window over every document, such that the input is the context of the target word and the document ID.
After training, each column in $\vec{D}$ represents the distributed encoding of the corresponding document.

The network is also able to learn a representation for new unseen documents by an inference step.
In this phase, the word matrix $\vec{W}$ and the prediction matrix $\vec{W'}$ are fixed and only the document vector is trained.
The paragraph vector model tends to perform better than non-distributed models, however since new documents are vectorized via inference, a bigger training corpus of documents is usually required.

\subsection{Latent Dirichlet Allocation}\label{sec:lda}

The \textit{Latent Dirichlet Allocation} (LDA) presented by \citeauthor{DBLP:journals/jmlr/BleiNJ03} in \citeyear{DBLP:journals/jmlr/BleiNJ03} \cite{DBLP:journals/jmlr/BleiNJ03} is generative statistical text model, which represents documents as a mixture of a small and fixed number of topics.
Topics are defined by probability distribution over all words in the vocabulary and are also learned by the model in an unsupervised manner.
The underlying assumption of the LDA model is that the documents were created by a statistical process, that sampled the words of the document from sampled topics.
A document is encoded as a vector, whose dimension is equal to the number of topics and each component indicates the probability that the corresponding topic distribution was chosen to sample a word in the document.
The distributions and therefore document encoding can be learned with statistical inference algorithms like Monte Carlo Simulation, Variational Bayes or maximum likelihood estimation using the words of each document.
Compared to the other text models, LDA can generate very compact documents encodings, since the documents are only described by their degree of affiliation to each topic.
Similar to the Bag of Words model, LDA does not consider the order of words in the document, which can be disadvantageous for some text corpora.

\section{Network Architecture and Training}

The encoded historical prefix event log is used to fit an LSTM model.
The LSTM network is designed to be trained with all prediction targets (next activity, next event time, case outcome and case cycle time) at once, in order to benefit from correlations between these.
In the basic variant, the network consists of an input layer, an shared LSTM layer, an specialized LSTM layer for each prediction target and an fully connected output layer for each target.
Furthermore, layer normalization \cite{DBLP:journals/corr/BaKH16} is applied after each LSTM layer, which standardizes the hidden output in order to speed up the training convergence.

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=\textwidth]{figures/network}
	\caption[LSTM model for text-aware process prediction]{LSTM model to simultaneously predict the next activity $(\vec{y_a})$, next event time$(y_t)$, case outcome $(\vec{y_o})$ and case cylce time $(y_c)$ for an encoded prefix trace $\vec{x_1}, \vec{x_2}, \dots, \vec{x_n}$.}
	\label{fig:network}
\end{figure}

The fully connected output layer uses a softmax activation function for the next activity and case outcome prediction to estimate the probability for each target value.
The softmax function normalizes a vector of real numbers into another vector of same dimension, such that all component are in the interval [0,1] and the sum of all component is equal to 1.
Hence, the transformed vector can be interpreted as a probability distribution, while keeping the proportions of the original vector.
The softmax function is described with
\begin{equation*}\label{key}
	\text{Softmax}(\vec{x})_i = \frac{\exp(x_i)}{\sum_{j=1}^{n} \exp(x_j)}  \; \text{for} \, i = 1, \dots, n \text{ and } \vec{x}=(x_1, \dots, x_n) \in \mathbb{R}^n.
\end{equation*}
The whole training set of encoded prefix traces is represented for efficiency by a 3-dimensional matrix of real values, where the three dimensions specify the prefix traces, the events per prefix trace and the features per event.
Since the prefix traces have different length, shorter traces are pre-padded \cite{DBLP:journals/corr/abs-1903-07288} with zeros vectors.
Hence, a prefix trace of encoded events $\vec{x_1}, \vec{x_2}, \dots, \vec{x_n}$  is represented in the training set by a 2-dimensional matrix $(\mathbf{0}, \dots, \mathbf{0},\vec{x_1}, \vec{x_2}, \dots, \vec{x_n})$, such that the zero vectors fill up shorter traces to the length of the longest trace in the training set.
All prefix traces together form the 3-dimensional training matrix.

The training is realized using a learning algorithm based on stochastic gradient descent and backpropagation through time (BPTT), which update the weights of the network using the update rules of the Adam optimizer with Nesterov momentum \cite{dozat2016incorporating}.
The loss for numerical prediction values $\hat{y}$ and the true value $y$ is the absolute error $\text{AE}(\hat{y},y)=|\hat{y} - y|$, while the loss for categorical prediction values is computed using the categorical cross entropy error $\text{CE}(\vec{\hat{y}}, \vec{y}) = - \sum_{i=1}^{k} y_i \cdot \log \hat{y}_i$.

\section{Predictive Business Processing Monitoring}

During online business process monitoring, predictions are realized by a forward-pass of the encoded, uncompleted traces through the LSTM model.
The component with the highest value of the softmax outputs for the next activity $(\vec{y_a})$ and the case outcome $(\vec{y_o})$ indicates the categorical prediction.
The output values for the next event time $(y_t)$ and case duration  $(y_c)$ are clipped to 0 for negative outputs and the normalization is reverted in order to compute the final prediction value.

With every new event, that is registered by the business process monitoring system, the prediction can be updated with another forward-pass through the model.
This guarantees a continuous forward-projection on running process instances, that is updated whenever new information in form of new events becomes available.